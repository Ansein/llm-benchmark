"""
åœºæ™¯Açš„LLMè¯„ä¼°å™¨
è¯„ä¼°LLMåœ¨ä¸ªæ€§åŒ–å®šä»·ä¸éšç§é€‰æ‹©åœºæ™¯ä¸‹çš„å†³ç­–èƒ½åŠ›
"""

import json
import numpy as np
from typing import Dict, List, Any, Tuple
from .llm_client import LLMClient
from src.scenarios.scenario_a_personalization import ScenarioAParams, solve_for_D


class ScenarioAEvaluator:
    """åœºæ™¯Aè¯„ä¼°å™¨"""
    
    def __init__(self, llm_client: LLMClient, ground_truth_path: str = "data/ground_truth/scenario_a_result.json"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            ground_truth_path: ground truthæ–‡ä»¶è·¯å¾„
        """
        self.llm_client = llm_client
        self.ground_truth_path = ground_truth_path
        
        # åŠ è½½ground truth
        with open(ground_truth_path, 'r', encoding='utf-8') as f:
            self.gt_data = json.load(f)
        
        self.params = ScenarioAParams(**self.gt_data["params"])
        self.gt_numeric = self.gt_data["gt_numeric"]
        self.gt_labels = self.gt_data["gt_labels"]
    
    def build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»Ÿæç¤º"""
        return """ä½ æ˜¯ä¸€ä¸ªç»æµå­¦ä¸“å®¶ï¼Œæ“…é•¿åˆ†æå¸‚åœºæœºåˆ¶å’Œéšç§å¤–éƒ¨æ€§é—®é¢˜ã€‚
ä½ éœ€è¦åœ¨ç»™å®šçš„åœºæ™¯ä¸‹åšå‡ºç†æ€§å†³ç­–ï¼Œå¹¶è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹ã€‚
è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬ã€‚"""
    
    def build_disclosure_prompt(self, consumer_id: int, current_disclosure_set: List[int]) -> str:
        """
        æ„å»ºæŠ«éœ²å†³ç­–æç¤ºï¼ˆé’ˆå¯¹å•ä¸ªæ¶ˆè´¹è€…ï¼‰
        
        Args:
            consumer_id: æ¶ˆè´¹è€…ID
            current_disclosure_set: å½“å‰å…¶ä»–æ¶ˆè´¹è€…çš„æŠ«éœ²é›†åˆ
        
        Returns:
            æç¤ºæ–‡æœ¬
        """
        theta_i = self.params.theta[consumer_id]
        c_privacy_i = self.params.c_privacy[consumer_id]
        n = self.params.n
        
        prompt = f"""
# åœºæ™¯æè¿°ï¼šä¸ªæ€§åŒ–å®šä»·ä¸éšç§é€‰æ‹©

ä½ æ˜¯æ¶ˆè´¹è€… {consumer_id}ï¼Œæ­£åœ¨è€ƒè™‘æ˜¯å¦å‘å¹³å°æŠ«éœ²ä½ çš„ä¸ªäººæ•°æ®ã€‚

## ä½ çš„ä¿¡æ¯
- ä½ å¯¹äº§å“çš„çœŸå®æ„¿ä»˜ï¼ˆwillingness to payï¼‰: {theta_i:.2f}
- ä½ çš„éšç§æˆæœ¬ï¼ˆæŠ«éœ²æ•°æ®ä¼šå¸¦æ¥çš„å¿ƒç†æˆæœ¬ï¼‰: {c_privacy_i:.3f}
- æ€»å…±æœ‰ {n} ä¸ªæ¶ˆè´¹è€…ï¼ˆåŒ…æ‹¬ä½ ï¼‰

## å¸‚åœºè§„åˆ™
1. **æŠ«éœ²æ•°æ®çš„æ¶ˆè´¹è€…**ï¼šå¹³å°ä¼šè¯†åˆ«ä½ çš„æ„¿ä»˜ï¼Œå¹¶å‘ä½ æ”¶å–ä¸ªæ€§åŒ–ä»·æ ¼ p_i = {theta_i:.2f}
   - ä½ çš„è´­ä¹°æ•ˆç”¨ = {theta_i:.2f} - {theta_i:.2f} - {c_privacy_i:.3f} = {-c_privacy_i:.3f}

2. **ä¸æŠ«éœ²æ•°æ®çš„æ¶ˆè´¹è€…**ï¼šå¹³å°æ— æ³•è¯†åˆ«ä½ ï¼Œåªèƒ½å¯¹æ‰€æœ‰æœªæŠ«éœ²è€…æ”¶å–ç»Ÿä¸€ä»·æ ¼ p_uniform
   - ä½ çš„è´­ä¹°æ•ˆç”¨ = {theta_i:.2f} - p_uniformï¼ˆå¦‚æœä½ é€‰æ‹©è´­ä¹°çš„è¯ï¼‰
   - æ˜¯å¦è´­ä¹°å–å†³äºï¼š{theta_i:.2f} >= p_uniform

3. **å…³é”®ç‚¹**ï¼šç»Ÿä¸€ä»·æ ¼ p_uniform å–å†³äºæœ‰å¤šå°‘äººæŠ«éœ²æ•°æ®
   - æŠ«éœ²çš„äººè¶Šå¤šï¼Œå¹³å°å¯¹æœªæŠ«éœ²è€…çš„ä¿¡æ¯è¶Šå°‘ï¼Œç»Ÿä¸€ä»·æ ¼å¯èƒ½ä¼šå˜åŒ–

## å½“å‰æƒ…å†µ
- å…¶ä»–æ¶ˆè´¹è€…ä¸­ï¼Œæœ‰ {len(current_disclosure_set)} äººé€‰æ‹©äº†æŠ«éœ²æ•°æ®
- ä½ éœ€è¦å†³å®šï¼šæ˜¯å¦æŠ«éœ²æ•°æ®ï¼Ÿ

## å†³ç­–ä»»åŠ¡
è¯·è¾“å‡ºä½ çš„å†³ç­–ï¼Œæ ¼å¼ä¸ºJSONï¼š
{{
  "decision": 0 æˆ– 1ï¼ˆ0=ä¸æŠ«éœ²ï¼Œ1=æŠ«éœ²ï¼‰ï¼Œ
  "rationale": "ç®€çŸ­è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹ï¼ˆå¯é€‰ï¼‰"
}}

è¯·åªè¾“å‡ºJSONï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡æœ¬ã€‚
"""
        return prompt
    
    def query_llm_disclosure_decision(
        self, 
        consumer_id: int, 
        current_disclosure_set: List[int],
        num_trials: int = 3
    ) -> Tuple[int, List[int]]:
        """
        æŸ¥è¯¢LLMçš„æŠ«éœ²å†³ç­–
        
        Args:
            consumer_id: æ¶ˆè´¹è€…ID
            current_disclosure_set: å½“å‰æŠ«éœ²é›†åˆ
            num_trials: é‡å¤æŸ¥è¯¢æ¬¡æ•°ï¼ˆç”¨äºè¯„ä¼°ç¨³å®šæ€§ï¼‰
        
        Returns:
            (å¤šæ•°æŠ•ç¥¨ç»“æœ, æ‰€æœ‰è¯•éªŒçš„å†³ç­–åˆ—è¡¨)
        """
        prompt = self.build_disclosure_prompt(consumer_id, current_disclosure_set)
        
        decisions = []
        for trial in range(num_trials):
            try:
                response = self.llm_client.generate_json([
                    {"role": "system", "content": self.build_system_prompt()},
                    {"role": "user", "content": prompt}
                ])
                
                decision = int(response["decision"])
                if decision not in [0, 1]:
                    print(f"  âš ï¸  è¯•éªŒ{trial+1}: æ— æ•ˆå†³ç­– {decision}ï¼Œé»˜è®¤ä¸º0")
                    decision = 0
                
                decisions.append(decision)
                
            except Exception as e:
                print(f"  âš ï¸  è¯•éªŒ{trial+1}å¤±è´¥: {e}")
                decisions.append(0)  # å¤±è´¥æ—¶é»˜è®¤ä¸æŠ«éœ²
        
        # å¤šæ•°æŠ•ç¥¨
        final_decision = 1 if sum(decisions) > len(decisions) / 2 else 0
        return final_decision, decisions
    
    def simulate_llm_equilibrium(self, num_trials: int = 3, max_iterations: int = 10) -> Dict[str, Any]:
        """
        æ¨¡æ‹ŸLLMä»£ç†è¾¾åˆ°çš„æŠ«éœ²å‡è¡¡
        
        ç­–ç•¥ï¼š
        1. ä»ç©ºé›†åˆå¼€å§‹
        2. æ¯è½®è®©æ¯ä¸ªæ¶ˆè´¹è€…ï¼ˆéšæœºé¡ºåºï¼‰é‡æ–°å†³ç­–
        3. é‡å¤ç›´åˆ°æ”¶æ•›æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        
        Args:
            num_trials: æ¯ä¸ªå†³ç­–çš„é‡å¤æ¬¡æ•°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¤– å¼€å§‹æ¨¡æ‹ŸLLMå‡è¡¡ (æ¨¡å‹: {self.llm_client.config_name})")
        print(f"{'='*60}")
        
        n = self.params.n
        disclosure_set = set()  # å½“å‰æŠ«éœ²é›†åˆ
        
        # è¿½è¸ªæ”¶æ•›è¿‡ç¨‹
        history = []
        
        for iteration in range(max_iterations):
            print(f"\n--- è¿­ä»£ {iteration + 1} ---")
            print(f"å½“å‰æŠ«éœ²é›†åˆ: {sorted(disclosure_set)}")
            
            # éšæœºé¡ºåºéå†æ¶ˆè´¹è€…
            consumers = list(range(n))
            np.random.shuffle(consumers)
            
            changed = False
            for consumer_id in consumers:
                # å…¶ä»–äººçš„æŠ«éœ²é›†åˆ
                others_disclosure = sorted([i for i in disclosure_set if i != consumer_id])
                
                # æŸ¥è¯¢LLMå†³ç­–
                print(f"\n  æ¶ˆè´¹è€… {consumer_id}: ", end="")
                decision, trials = self.query_llm_disclosure_decision(
                    consumer_id, 
                    others_disclosure,
                    num_trials=num_trials
                )
                
                print(f"å†³ç­–={decision}, è¯•éªŒç»“æœ={trials}")
                
                # æ›´æ–°æŠ«éœ²é›†åˆ
                if decision == 1 and consumer_id not in disclosure_set:
                    disclosure_set.add(consumer_id)
                    changed = True
                    print(f"    âœ… æ¶ˆè´¹è€…{consumer_id}åŠ å…¥æŠ«éœ²é›†åˆ")
                elif decision == 0 and consumer_id in disclosure_set:
                    disclosure_set.remove(consumer_id)
                    changed = True
                    print(f"    âŒ æ¶ˆè´¹è€…{consumer_id}ç¦»å¼€æŠ«éœ²é›†åˆ")
            
            # è®°å½•å†å²
            history.append(sorted(disclosure_set))
            
            # æ£€æŸ¥æ”¶æ•›
            if not changed:
                print(f"\nâœ… åœ¨ç¬¬{iteration + 1}è½®è¾¾åˆ°æ”¶æ•›ï¼")
                break
        
        # è®¡ç®—LLMå‡è¡¡ä¸‹çš„ç»“æœ
        llm_disclosure_set = sorted(disclosure_set)
        llm_outcome = solve_for_D(self.params, set(llm_disclosure_set))
        
        # ä¸ground truthæ¯”è¾ƒ
        gt_disclosure_set = sorted(self.gt_numeric["eq_disclosure_set"])
        gt_profit = self.gt_numeric["eq_profit"]
        gt_CS = self.gt_numeric["eq_CS"]
        gt_W = self.gt_numeric["eq_W"]
        
        # è®¡ç®—åå·®
        results = {
            "model_name": self.llm_client.config_name,
            "llm_disclosure_set": llm_disclosure_set,
            "gt_disclosure_set": gt_disclosure_set,
            "convergence_history": history,
            "converged": len(history) < max_iterations,
            "iterations": len(history),
            "metrics": {
                "llm": {
                    "profit": llm_outcome.total_profit,
                    "consumer_surplus": llm_outcome.consumer_surplus,
                    "welfare": llm_outcome.welfare,
                    "disclosure_rate": len(llm_disclosure_set) / n
                },
                "ground_truth": {
                    "profit": gt_profit,
                    "consumer_surplus": gt_CS,
                    "welfare": gt_W,
                    "disclosure_rate": len(gt_disclosure_set) / n
                },
                "deviations": {
                    "profit_mae": abs(llm_outcome.total_profit - gt_profit),
                    "cs_mae": abs(llm_outcome.consumer_surplus - gt_CS),
                    "welfare_mae": abs(llm_outcome.welfare - gt_W),
                    "disclosure_rate_mae": abs(len(llm_disclosure_set) / n - len(gt_disclosure_set) / n)
                }
            },
            "labels": {
                "llm_disclosure_rate_bucket": self._bucket_disclosure_rate(len(llm_disclosure_set) / n),
                "gt_disclosure_rate_bucket": self.gt_labels["disclosure_rate_bucket"],
                "llm_over_disclosure": 1 if len(llm_disclosure_set) > len(self.gt_numeric["fb_disclosure_set"]) else 0,
                "gt_over_disclosure": self.gt_labels["over_disclosure"]
            }
        }
        
        return results
    
    def _bucket_disclosure_rate(self, rate: float) -> str:
        """å°†æŠ«éœ²ç‡åˆ†æ¡¶"""
        if rate < 0.33:
            return "low"
        elif rate < 0.67:
            return "medium"
        else:
            return "high"
    
    def print_evaluation_summary(self, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print(f"{'='*60}")
        
        print(f"\nã€æŠ«éœ²é›†åˆæ¯”è¾ƒã€‘")
        print(f"  LLMå‡è¡¡: {results['llm_disclosure_set']}")
        print(f"  ç†è®ºå‡è¡¡: {results['gt_disclosure_set']}")
        print(f"  æ”¶æ•›æƒ…å†µ: {'âœ… å·²æ”¶æ•›' if results['converged'] else 'âŒ æœªæ”¶æ•›'} (è¿­ä»£{results['iterations']}æ¬¡)")
        
        print(f"\nã€å…³é”®æŒ‡æ ‡ã€‘")
        llm_m = results['metrics']['llm']
        gt_m = results['metrics']['ground_truth']
        dev_m = results['metrics']['deviations']
        
        print(f"  å¹³å°åˆ©æ¶¦:     LLM={llm_m['profit']:.3f}  |  GT={gt_m['profit']:.3f}  |  MAE={dev_m['profit_mae']:.3f}")
        print(f"  æ¶ˆè´¹è€…å‰©ä½™:   LLM={llm_m['consumer_surplus']:.3f}  |  GT={gt_m['consumer_surplus']:.3f}  |  MAE={dev_m['cs_mae']:.3f}")
        print(f"  ç¤¾ä¼šç¦åˆ©:     LLM={llm_m['welfare']:.3f}  |  GT={gt_m['welfare']:.3f}  |  MAE={dev_m['welfare_mae']:.3f}")
        print(f"  æŠ«éœ²ç‡:       LLM={llm_m['disclosure_rate']:.2%}  |  GT={gt_m['disclosure_rate']:.2%}  |  MAE={dev_m['disclosure_rate_mae']:.2%}")
        
        print(f"\nã€æ ‡ç­¾ä¸€è‡´æ€§ã€‘")
        llm_l = results['labels']
        print(f"  æŠ«éœ²ç‡åˆ†æ¡¶:   LLM={llm_l['llm_disclosure_rate_bucket']}  |  GT={llm_l['gt_disclosure_rate_bucket']}  |  {'âœ…' if llm_l['llm_disclosure_rate_bucket'] == llm_l['gt_disclosure_rate_bucket'] else 'âŒ'}")
        print(f"  è¿‡åº¦æŠ«éœ²:     LLM={llm_l['llm_over_disclosure']}  |  GT={llm_l['gt_over_disclosure']}  |  {'âœ…' if llm_l['llm_over_disclosure'] == llm_l['gt_over_disclosure'] else 'âŒ'}")
    
    def save_results(self, results: Dict[str, Any], output_path: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """æµ‹è¯•è¯„ä¼°å™¨"""
    from .llm_client import create_llm_client
    import os
    
    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = create_llm_client("deepseek-v3.2")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ScenarioAEvaluator(llm_client)
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.simulate_llm_equilibrium(num_trials=3, max_iterations=5)
    
    # æ‰“å°æ‘˜è¦
    evaluator.print_evaluation_summary(results)
    
    # åˆ›å»ºåœºæ™¯Aä¸“å±è¾“å‡ºç›®å½•
    output_dir = "evaluation_results/scenario_a"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ç»“æœ
    evaluator.save_results(results, f"{output_dir}/eval_scenario_a_{llm_client.config_name}.json")


if __name__ == "__main__":
    main()
