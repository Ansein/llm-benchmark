"""
åœºæ™¯Cä¸»è¯„ä¼°å™¨

æ”¯æŒ4ç§é…ç½®çš„è¯„ä¼°ï¼š
- é…ç½®Aï¼šç†æ€§Ã—ç†æ€§ï¼ˆç†è®ºåŸºå‡†ï¼‰
- é…ç½®Bï¼šç†æ€§ä¸­ä»‹Ã—LLMæ¶ˆè´¹è€…
- é…ç½®Cï¼šLLMä¸­ä»‹Ã—ç†æ€§æ¶ˆè´¹è€…
- é…ç½®Dï¼šLLMä¸­ä»‹Ã—LLMæ¶ˆè´¹è€…

æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯å®Œå…¨é‡åŒ–çš„ã€å®¢è§‚çš„ã€‚
"""

import sys
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Callable, Optional, Tuple
from dataclasses import dataclass

# å¤„ç†ç›´æ¥è¿è¡Œå’Œæ¨¡å—å¯¼å…¥ä¸¤ç§æƒ…å†µ
if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œï¼šæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
    project_root = Path(__file__).parent.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

from src.scenarios.scenario_c_social_data import (
    ScenarioCParams,
    ConsumerData,
    simulate_market_outcome,
    compute_rational_participation_rate_ex_ante,
    evaluate_intermediary_strategy,
    generate_consumer_data
)
from src.evaluators.scenario_c_metrics import (
    compute_participation_metrics,
    compute_market_metrics,
    compute_inequality_metrics,
    compute_strategy_metrics,
    compute_profit_metrics,
    compute_ranking_metrics,
    compute_interaction_metrics
)


@dataclass
class LLMConsumerAgent:
    """LLMæ¶ˆè´¹è€…ä»£ç†çš„æŠ½è±¡æ¥å£"""
    
    def decide(
        self,
        consumer_params: Dict,
        m: float,
        anonymization: str,
        context: Optional[Dict] = None
    ) -> bool:
        """
        æ¶ˆè´¹è€…å†³ç­–
        
        Args:
            consumer_params: æ¶ˆè´¹è€…å‚æ•° {theta_i, tau_i, w_i (å¯é€‰)}
            m: è¡¥å¿é‡‘é¢
            anonymization: åŒ¿ååŒ–ç­–ç•¥
            context: é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æ˜¯å¦å‚ä¸æ•°æ®åˆ†äº«
        """
        raise NotImplementedError


@dataclass
class LLMIntermediaryAgent:
    """LLMä¸­ä»‹ä»£ç†çš„æŠ½è±¡æ¥å£"""
    
    def choose_strategy(
        self,
        market_params: Dict,
        context: Optional[Dict] = None
    ) -> Tuple[float, str]:
        """
        ä¸­ä»‹é€‰æ‹©ç­–ç•¥
        
        Args:
            market_params: å¸‚åœºå‚æ•° {N, mu_theta, sigma_theta, tau_mean, tau_std, ...}
            context: é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            (m, anonymization) å…ƒç»„
        """
        raise NotImplementedError


class ScenarioCEvaluator:
    """åœºæ™¯Cè¯„ä¼°å™¨"""
    
    def __init__(self, ground_truth_path: str):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            ground_truth_path: Ground Truthæ–‡ä»¶è·¯å¾„ï¼ˆé…ç½®Aï¼‰
        """
        self.gt_A = self.load_ground_truth(ground_truth_path)
        self.params_base = self._extract_params_base()
        
    def load_ground_truth(self, path: str) -> Dict:
        """åŠ è½½Ground Truthæ–‡ä»¶"""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _extract_params_base(self) -> Dict:
        """ä»GTä¸­æå–åŸºç¡€å‚æ•°"""
        # GTæ–‡ä»¶ä¸­ç›´æ¥æœ‰params_baseå­—æ®µ
        return dict(self.gt_A['params_base'])
    
    def _get_sample_consumers(self) -> List[Dict]:
        """
        è·å–æ ·æœ¬æ¶ˆè´¹è€…æ•°æ®ï¼ˆåŒ…æ‹¬ç”Ÿæˆtauå€¼ï¼‰
        
        Returns:
            æ¶ˆè´¹è€…å‚æ•°åˆ—è¡¨
        """
        sample_data = self.gt_A['sample_data']
        N = self.params_base['N']
        
        # ç”Ÿæˆtauå€¼ï¼ˆä½¿ç”¨GTçš„seedç¡®ä¿å¯å¤ç°ï¼‰
        rng = np.random.default_rng(self.params_base['seed'] + 1000)
        if self.params_base['tau_dist'] == 'normal':
            tau_values = rng.normal(
                self.params_base['tau_mean'],
                self.params_base['tau_std'],
                N
            )
        elif self.params_base['tau_dist'] == 'uniform':
            a = self.params_base['tau_mean'] - np.sqrt(3) * self.params_base['tau_std']
            b = self.params_base['tau_mean'] + np.sqrt(3) * self.params_base['tau_std']
            tau_values = rng.uniform(a, b, N)
        else:
            tau_values = np.zeros(N)
        
        consumers = []
        for i in range(N):
            consumer = {
                'tau_i': float(tau_values[i]),
            }
            
            # æ ¹æ®data_structureæ·»åŠ thetaå’Œw
            if self.params_base['data_structure'] == 'common_preferences':
                consumer['theta_i'] = float(sample_data['theta'])
                consumer['w_i'] = float(sample_data['w'][i])
            elif self.params_base['data_structure'] == 'common_experience':
                consumer['theta_i'] = float(sample_data['w'][i])  # åœ¨common_experienceä¸­ï¼Œwå®é™…ä¸Šæ˜¯theta
                consumer['w_i'] = float(sample_data['w'][i])
            
            consumers.append(consumer)
        
        return consumers
    
    def _get_theory_decisions(self, delta_u: float, consumers: List[Dict]) -> np.ndarray:
        """
        è®¡ç®—ç†è®ºå†³ç­–
        
        Args:
            delta_u: å‚ä¸vsæ‹’ç»çš„æ•ˆç”¨å·®
            consumers: æ¶ˆè´¹è€…å‚æ•°åˆ—è¡¨
        
        Returns:
            ç†è®ºå†³ç­–æ•°ç»„ï¼ˆNä¸ªå¸ƒå°”å€¼ï¼‰
        """
        tau_values = np.array([c['tau_i'] for c in consumers])
        return tau_values <= delta_u
    
    def evaluate_config_B(
        self,
        llm_consumer_agent: Callable,
        verbose: bool = True
    ) -> Dict:
        """
        é…ç½®Bï¼šç†æ€§ä¸­ä»‹ Ã— LLMæ¶ˆè´¹è€…
        
        Args:
            llm_consumer_agent: LLMæ¶ˆè´¹è€…ä»£ç†ï¼ˆå‡½æ•°æˆ–å¯¹è±¡ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if verbose:
            print("\n" + "="*70)
            print("é…ç½®Bï¼šç†æ€§ä¸­ä»‹ Ã— LLMæ¶ˆè´¹è€…")
            print("="*70)
        
        # 1. è·å–ç†è®ºæœ€ä¼˜ç­–ç•¥
        m_star = self.gt_A['optimal_strategy']['m_star']
        anon_star = self.gt_A['optimal_strategy']['anonymization_star']
        r_star = self.gt_A['optimal_strategy']['r_star']
        delta_u = self.gt_A['optimal_strategy']['delta_u_star']
        
        if verbose:
            print(f"\nç†è®ºæœ€ä¼˜ç­–ç•¥: m*={m_star:.4f}, {anon_star}")
            print(f"ç†è®ºå‚ä¸ç‡: r*={r_star:.4f}")
        
        # 2. è·å–æ¶ˆè´¹è€…æ•°æ®
        consumers = self._get_sample_consumers()
        N = self.params_base['N']
        
        # 3. LLMå†³ç­–
        if verbose:
            print(f"\næ­£åœ¨æ”¶é›†{N}ä¸ªLLMæ¶ˆè´¹è€…çš„å†³ç­–...")
        
        llm_decisions = []
        for consumer_params in consumers:
            # è°ƒç”¨LLMä»£ç†
            if callable(llm_consumer_agent):
                decision = llm_consumer_agent(
                    consumer_params=consumer_params,
                    m=m_star,
                    anonymization=anon_star
                )
            else:
                decision = llm_consumer_agent.decide(
                    consumer_params=consumer_params,
                    m=m_star,
                    anonymization=anon_star
                )
            
            llm_decisions.append(bool(decision))
        
        llm_decisions = np.array(llm_decisions)
        r_llm = float(np.mean(llm_decisions))
        
        if verbose:
            print(f"LLMå‚ä¸ç‡: r_llm={r_llm:.4f}")
            print(f"ç†è®ºå‚ä¸ç‡: r*={r_star:.4f}")
            print(f"åå·®: {abs(r_llm - r_star):.4f}")
        
        # 4. è®¡ç®—ç†è®ºå†³ç­–
        theory_decisions = self._get_theory_decisions(delta_u, consumers)
        
        # 5. è®¡ç®—å¸‚åœºç»“æœï¼ˆä½¿ç”¨LLMçš„å‚ä¸å†³ç­–ï¼‰
        params = ScenarioCParams(
            m=m_star,
            anonymization=anon_star,
            **self.params_base
        )
        
        # ç”Ÿæˆæ¶ˆè´¹è€…æ•°æ®
        rng = np.random.default_rng(self.params_base['seed'])
        consumer_data = generate_consumer_data(params, rng=rng)
        
        # æ¨¡æ‹Ÿå¸‚åœº
        outcome_llm = simulate_market_outcome(
            consumer_data,
            llm_decisions,
            params,
            producer_info_mode="with_data",
            m0=self.gt_A['data_transaction']['m_0'],
            rng=rng
        )
        
        # 6. æå–ç†è®ºå¸‚åœºç»“æœ
        outcome_theory = {
            'social_welfare': self.gt_A['equilibrium']['social_welfare'],
            'consumer_surplus': self.gt_A['equilibrium']['consumer_surplus'],
            'producer_profit': self.gt_A['equilibrium']['producer_profit'],
            'intermediary_profit': self.gt_A['equilibrium']['intermediary_profit'],
            'gini_coefficient': self.gt_A['equilibrium']['gini_coefficient'],
            'price_variance': self.gt_A['equilibrium'].get('price_variance', 0.0),
            'price_discrimination_index': self.gt_A['equilibrium'].get('price_discrimination_index', 0.0),
        }
        
        outcome_llm_dict = {
            'social_welfare': outcome_llm.social_welfare,
            'consumer_surplus': outcome_llm.consumer_surplus,
            'producer_profit': outcome_llm.producer_profit,
            'intermediary_profit': outcome_llm.intermediary_profit,
            'gini_coefficient': outcome_llm.gini_coefficient,
            'price_variance': outcome_llm.price_variance,
            'price_discrimination_index': outcome_llm.price_discrimination_index,
        }
        
        # 7. è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        metrics = {
            "config": "B_rational_intermediary_llm_consumer",
            "participation": compute_participation_metrics(
                llm_decisions,
                theory_decisions,
                r_star
            ),
            "market": compute_market_metrics(
                outcome_llm_dict,
                outcome_theory
            ),
            "inequality": compute_inequality_metrics(
                outcome_llm_dict,
                outcome_theory
            ),
        }
        
        if verbose:
            print(f"\nå…³é”®æŒ‡æ ‡:")
            print(f"  å‚ä¸ç‡è¯¯å·®: {metrics['participation']['r_relative_error']:.2%}")
            print(f"  ä¸ªä½“å‡†ç¡®ç‡: {metrics['participation']['individual_accuracy']:.2%}")
            print(f"  ç¦åˆ©æ¯”ç‡: {metrics['market']['social_welfare_ratio']:.4f}")
            print(f"  ç¦åˆ©æŸå¤±: {metrics['market']['welfare_loss_percent']:.2f}%")
        
        return metrics
    
    def evaluate_config_C(
        self,
        llm_intermediary_agent: Callable,
        verbose: bool = True
    ) -> Dict:
        """
        é…ç½®Cï¼šLLMä¸­ä»‹ Ã— ç†æ€§æ¶ˆè´¹è€…
        
        Args:
            llm_intermediary_agent: LLMä¸­ä»‹ä»£ç†ï¼ˆå‡½æ•°æˆ–å¯¹è±¡ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if verbose:
            print("\n" + "="*70)
            print("é…ç½®Cï¼šLLMä¸­ä»‹ Ã— ç†æ€§æ¶ˆè´¹è€…")
            print("="*70)
        
        # 1. è·å–ç†è®ºæœ€ä¼˜ç­–ç•¥
        m_star = self.gt_A['optimal_strategy']['m_star']
        anon_star = self.gt_A['optimal_strategy']['anonymization_star']
        profit_star = self.gt_A['optimal_strategy']['intermediary_profit_star']
        
        if verbose:
            print(f"\nç†è®ºæœ€ä¼˜ç­–ç•¥: m*={m_star:.4f}, {anon_star}")
            print(f"ç†è®ºæœ€ä¼˜åˆ©æ¶¦: {profit_star:.4f}")
        
        # 2. LLMé€‰æ‹©ç­–ç•¥
        if verbose:
            print(f"\nè¯·LLMä¸­ä»‹é€‰æ‹©ç­–ç•¥...")
        
        market_params = {
            'N': self.params_base['N'],
            'mu_theta': self.params_base['mu_theta'],
            'sigma_theta': self.params_base['sigma_theta'],
            'tau_mean': self.params_base['tau_mean'],
            'tau_std': self.params_base['tau_std'],
            'data_structure': self.params_base['data_structure'],
        }
        
        if callable(llm_intermediary_agent):
            m_llm, anon_llm = llm_intermediary_agent(market_params=market_params)
        else:
            m_llm, anon_llm = llm_intermediary_agent.choose_strategy(market_params=market_params)
        
        if verbose:
            print(f"LLMé€‰æ‹©: m={m_llm:.4f}, {anon_llm}")
        
        # 3. è®¡ç®—ç†æ€§æ¶ˆè´¹è€…çš„ååº”ï¼ˆä½¿ç”¨LLMçš„ç­–ç•¥ï¼‰
        if verbose:
            print(f"\nè®¡ç®—ç†æ€§æ¶ˆè´¹è€…å¯¹LLMç­–ç•¥çš„ååº”...")
        
        result_llm = evaluate_intermediary_strategy(
            m=m_llm,
            anonymization=anon_llm,
            params_base=self.params_base,
            num_mc_samples=50,
            max_iter=100,
            tol=1e-3,
            seed=self.params_base['seed']
        )
        
        profit_llm = result_llm.intermediary_profit
        r_given_llm = result_llm.r_star
        
        if verbose:
            print(f"ç†æ€§å‚ä¸ç‡(ç»™å®šLLMç­–ç•¥): r*={r_given_llm:.4f}")
            print(f"LLMç­–ç•¥åˆ©æ¶¦: {profit_llm:.4f}")
            print(f"ç†è®ºæœ€ä¼˜åˆ©æ¶¦: {profit_star:.4f}")
            print(f"åˆ©æ¶¦æ•ˆç‡: {profit_llm / profit_star:.2%}")
        
        # 4. è®¡ç®—å¸‚åœºç»“æœ
        outcome_llm = {
            'social_welfare': result_llm.social_welfare,
            'consumer_surplus': result_llm.consumer_surplus,
            'producer_profit': result_llm.producer_profit_with_data,
            'intermediary_profit': result_llm.intermediary_profit,
        }
        
        outcome_theory = {
            'social_welfare': self.gt_A['equilibrium']['social_welfare'],
            'consumer_surplus': self.gt_A['equilibrium']['consumer_surplus'],
            'producer_profit': self.gt_A['equilibrium']['producer_profit'],
            'intermediary_profit': self.gt_A['equilibrium']['intermediary_profit'],
        }
        
        # 5. è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        cost_llm = m_llm * result_llm.num_participants
        cost_theory = m_star * self.gt_A['optimal_strategy'].get('num_participants_expected', 0)
        
        metrics = {
            "config": "C_llm_intermediary_rational_consumer",
            "strategy": compute_strategy_metrics(
                m_llm, anon_llm,
                m_star, anon_star
            ),
            "profit": compute_profit_metrics(
                profit_llm, profit_star,
                cost_llm, cost_theory
            ),
            "market": compute_market_metrics(
                outcome_llm,
                outcome_theory
            ),
            "participation_given_llm_strategy": {
                "r_given_llm": r_given_llm,
                "r_optimal": self.gt_A['optimal_strategy']['r_star'],
                "r_ratio": r_given_llm / self.gt_A['optimal_strategy']['r_star'],
            }
        }
        
        if verbose:
            print(f"\nå…³é”®æŒ‡æ ‡:")
            print(f"  ç­–ç•¥mè¯¯å·®: {metrics['strategy']['m_relative_error']:.2%}")
            print(f"  åŒ¿ååŒ–åŒ¹é…: {'âœ“' if metrics['strategy']['anon_match'] else 'âœ—'}")
            print(f"  åˆ©æ¶¦æ•ˆç‡: {metrics['profit']['profit_ratio']:.2%}")
            print(f"  åˆ©æ¶¦æŸå¤±: {metrics['profit']['profit_loss_percent']:.2f}%")
        
        return metrics
    
    def evaluate_config_D(
        self,
        llm_intermediary_agent: Callable,
        llm_consumer_agent: Callable,
        verbose: bool = True
    ) -> Dict:
        """
        é…ç½®Dï¼šLLMä¸­ä»‹ Ã— LLMæ¶ˆè´¹è€…
        
        Args:
            llm_intermediary_agent: LLMä¸­ä»‹ä»£ç†
            llm_consumer_agent: LLMæ¶ˆè´¹è€…ä»£ç†
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if verbose:
            print("\n" + "="*70)
            print("é…ç½®Dï¼šLLMä¸­ä»‹ Ã— LLMæ¶ˆè´¹è€…")
            print("="*70)
        
        # 1. LLMä¸­ä»‹é€‰æ‹©ç­–ç•¥
        if verbose:
            print(f"\næ­¥éª¤1: LLMä¸­ä»‹é€‰æ‹©ç­–ç•¥...")
        
        market_params = {
            'N': self.params_base['N'],
            'mu_theta': self.params_base['mu_theta'],
            'sigma_theta': self.params_base['sigma_theta'],
            'tau_mean': self.params_base['tau_mean'],
            'tau_std': self.params_base['tau_std'],
            'data_structure': self.params_base['data_structure'],
        }
        
        if callable(llm_intermediary_agent):
            m_llm, anon_llm = llm_intermediary_agent(market_params=market_params)
        else:
            m_llm, anon_llm = llm_intermediary_agent.choose_strategy(market_params=market_params)
        
        if verbose:
            print(f"LLMä¸­ä»‹é€‰æ‹©: m={m_llm:.4f}, {anon_llm}")
        
        # 2. LLMæ¶ˆè´¹è€…ååº”
        if verbose:
            print(f"\næ­¥éª¤2: æ”¶é›†LLMæ¶ˆè´¹è€…å†³ç­–...")
        
        consumers = self._get_sample_consumers()
        N = self.params_base['N']
        
        llm_decisions = []
        for consumer_params in consumers:
            if callable(llm_consumer_agent):
                decision = llm_consumer_agent(
                    consumer_params=consumer_params,
                    m=m_llm,
                    anonymization=anon_llm
                )
            else:
                decision = llm_consumer_agent.decide(
                    consumer_params=consumer_params,
                    m=m_llm,
                    anonymization=anon_llm
                )
            
            llm_decisions.append(bool(decision))
        
        llm_decisions = np.array(llm_decisions)
        r_llm = float(np.mean(llm_decisions))
        
        if verbose:
            print(f"LLMæ¶ˆè´¹è€…å‚ä¸ç‡: r={r_llm:.4f}")
        
        # 3. è®¡ç®—å¸‚åœºç»“æœ
        if verbose:
            print(f"\næ­¥éª¤3: è®¡ç®—å¸‚åœºç»“æœ...")
        
        params = ScenarioCParams(
            m=m_llm,
            anonymization=anon_llm,
            **self.params_base
        )
        
        rng = np.random.default_rng(self.params_base['seed'])
        consumer_data = generate_consumer_data(params, rng=rng)
        
        # ä¼°ç®—m_0ï¼ˆç®€åŒ–ï¼šä½¿ç”¨ç†è®ºæ¨¡å‹ï¼‰
        from src.scenarios.scenario_c_social_data import estimate_m0_mc
        
        def participation_rule(p, world, rng):
            # ä½¿ç”¨LLMå†³ç­–ä½œä¸ºå‚ä¸è§„åˆ™çš„è¿‘ä¼¼
            return llm_decisions
        
        m_0_D, _, _, _ = estimate_m0_mc(
            params=params,
            participation_rule=participation_rule,
            T=100,
            beta=1.0,
            seed=self.params_base['seed']
        )
        
        outcome_D = simulate_market_outcome(
            consumer_data,
            llm_decisions,
            params,
            producer_info_mode="with_data",
            m0=m_0_D,
            rng=rng
        )
        
        outcome_D_dict = {
            'social_welfare': outcome_D.social_welfare,
            'consumer_surplus': outcome_D.consumer_surplus,
            'producer_profit': outcome_D.producer_profit,
            'intermediary_profit': outcome_D.intermediary_profit,
        }
        
        # 4. æå–é…ç½®Açš„ç»“æœ
        outcome_A = {
            'social_welfare': self.gt_A['equilibrium']['social_welfare'],
            'consumer_surplus': self.gt_A['equilibrium']['consumer_surplus'],
            'producer_profit': self.gt_A['equilibrium']['producer_profit'],
            'intermediary_profit': self.gt_A['equilibrium']['intermediary_profit'],
        }
        
        # 5. è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        metrics = {
            "config": "D_llm_intermediary_llm_consumer",
            "strategy": {
                "m_llm": m_llm,
                "anon_llm": anon_llm,
                "r_llm": r_llm,
            },
            "vs_theory": {
                "m_error": abs(m_llm - self.gt_A['optimal_strategy']['m_star']),
                "anon_match": int(anon_llm == self.gt_A['optimal_strategy']['anonymization_star']),
                "r_error": abs(r_llm - self.gt_A['optimal_strategy']['r_star']),
            },
            "market": compute_market_metrics(outcome_D_dict, outcome_A),
            "interaction": compute_interaction_metrics(
                outcome_D_dict,
                outcome_A
            )
        }
        
        if verbose:
            print(f"\nå…³é”®æŒ‡æ ‡:")
            print(f"  vsç†è®ºæœ€ä¼˜:")
            print(f"    ç­–ç•¥mè¯¯å·®: {metrics['vs_theory']['m_error']:.4f}")
            print(f"    å‚ä¸ç‡è¯¯å·®: {metrics['vs_theory']['r_error']:.4f}")
            print(f"    ç¦åˆ©æ¯”ç‡: {metrics['market']['social_welfare_ratio']:.4f}")
            print(f"    ç¦åˆ©æŸå¤±: {metrics['market']['welfare_loss_percent']:.2f}%")
            print(f"  äº¤äº’æŒ‡æ ‡:")
            print(f"    å‰¥å‰ŠæŒ‡æ ‡: {metrics['interaction']['exploitation_indicator']:.4f}")
        
        return metrics
    
    def generate_report(
        self,
        results_B: Dict = None,
        results_C: Dict = None,
        results_D: Dict = None,
        output_path: str = None
    ) -> pd.DataFrame:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            results_B: é…ç½®Bçš„è¯„ä¼°ç»“æœ
            results_C: é…ç½®Cçš„è¯„ä¼°ç»“æœ
            results_D: é…ç½®Dçš„è¯„ä¼°ç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            DataFrameæ ¼å¼çš„æŠ¥å‘Š
        """
        rows = []
        
        # é…ç½®Aï¼ˆç†è®ºåŸºå‡†ï¼‰
        row_A = {
            "Config": "A (Theory)",
            "Intermediary": "Rational",
            "Consumers": "Rational",
            "m": self.gt_A['optimal_strategy']['m_star'],
            "Anonymization": self.gt_A['optimal_strategy']['anonymization_star'],
            "Participation Rate": self.gt_A['optimal_strategy']['r_star'],
            "Social Welfare": self.gt_A['equilibrium']['social_welfare'],
            "Consumer Surplus": self.gt_A['equilibrium']['consumer_surplus'],
            "Producer Profit": self.gt_A['equilibrium']['producer_profit'],
            "Intermediary Profit": self.gt_A['equilibrium']['intermediary_profit'],
            "Welfare Loss (%)": 0.0,
        }
        rows.append(row_A)
        
        # é…ç½®B
        if results_B:
            row_B = {
                "Config": "B",
                "Intermediary": "Rational",
                "Consumers": "LLM",
                "m": self.gt_A['optimal_strategy']['m_star'],
                "Anonymization": self.gt_A['optimal_strategy']['anonymization_star'],
                "Participation Rate": results_B['participation']['r_llm'],
                "Social Welfare": results_B['market']['social_welfare_llm'],
                "Consumer Surplus": results_B['market']['consumer_surplus_llm'],
                "Producer Profit": results_B['market']['producer_profit_llm'],
                "Intermediary Profit": results_B['market']['intermediary_profit_llm'],
                "Welfare Loss (%)": results_B['market']['welfare_loss_percent'],
            }
            rows.append(row_B)
        
        # é…ç½®C
        if results_C:
            row_C = {
                "Config": "C",
                "Intermediary": "LLM",
                "Consumers": "Rational",
                "m": results_C['strategy']['m_llm'],
                "Anonymization": results_C['strategy']['anon_llm'],
                "Participation Rate": results_C['participation_given_llm_strategy']['r_given_llm'],
                "Social Welfare": results_C['market']['social_welfare_llm'],
                "Consumer Surplus": results_C['market']['consumer_surplus_llm'],
                "Producer Profit": results_C['market']['producer_profit_llm'],
                "Intermediary Profit": results_C['profit']['profit_llm'],
                "Welfare Loss (%)": results_C['market']['welfare_loss_percent'],
            }
            rows.append(row_C)
        
        # é…ç½®D
        if results_D:
            row_D = {
                "Config": "D",
                "Intermediary": "LLM",
                "Consumers": "LLM",
                "m": results_D['strategy']['m_llm'],
                "Anonymization": results_D['strategy']['anon_llm'],
                "Participation Rate": results_D['strategy']['r_llm'],
                "Social Welfare": results_D['market']['social_welfare_llm'],
                "Consumer Surplus": results_D['market']['consumer_surplus_llm'],
                "Producer Profit": results_D['market']['producer_profit_llm'],
                "Intermediary Profit": results_D['market']['intermediary_profit_llm'],
                "Welfare Loss (%)": results_D['market']['welfare_loss_percent'],
            }
            rows.append(row_D)
        
        df = pd.DataFrame(rows)
        
        if output_path:
            df.to_csv(output_path, index=False)
            print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return df


# ============================================================================
# ç›´æ¥è¿è¡Œç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œè¯„ä¼°å™¨
    
    ä½¿ç”¨configs/model_configs.jsonä¸­é…ç½®çš„çœŸå®LLMæ¨¡å‹
    """
    import sys
    import io
    from pathlib import Path
    from openai import OpenAI
    import re
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    project_root = Path(__file__).parent.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    
    print("=" * 70)
    print("åœºæ™¯Cè¯„ä¼°å™¨ - ä½¿ç”¨çœŸå®LLMæ¨¡å‹")
    print("=" * 70)
    
    # ========================================================================
    # åŠ è½½æ¨¡å‹é…ç½®
    # ========================================================================
    config_path = "configs/model_configs.json"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            model_configs = json.load(f)
        print(f"\nâœ… æˆåŠŸåŠ è½½æ¨¡å‹é…ç½®: {config_path}")
        print(f"å¯ç”¨æ¨¡å‹: {[cfg['config_name'] for cfg in model_configs]}")
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {config_path}")
        sys.exit(1)
    
    # ========================================================================
    # é€‰æ‹©è¦è¯„ä¼°çš„æ¨¡å‹ï¼ˆç›´æ¥æŒ‡å®šæ¨¡å‹åç§°ï¼‰
    # ========================================================================
    # ä¿®æ”¹è¿™é‡Œæ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹
    TARGET_MODEL = "gpt-4.1-mini"  # å¯é€‰: grok-3-mini, gpt-4.1-mini, deepseek-v3, gemini-2.5-flash
    
    selected_model_config = None
    for config in model_configs:
        if config['config_name'] == TARGET_MODEL:
            selected_model_config = config
            break
    
    if selected_model_config is None:
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®: {TARGET_MODEL}")
        print(f"å¯ç”¨æ¨¡å‹: {[cfg['config_name'] for cfg in model_configs]}")
        sys.exit(1)
    
    model_name = selected_model_config['config_name']
    print(f"\nğŸ¯ é€‰æ‹©æ¨¡å‹: {model_name}")
    
    # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
    client = OpenAI(
        api_key=selected_model_config['api_key'],
        **selected_model_config.get('client_args', {})
    )
    
    # ========================================================================
    # çœŸå®LLMä»£ç†å‡½æ•°
    # ========================================================================
    
    def create_llm_consumer(client, model_config):
        """åˆ›å»ºLLMæ¶ˆè´¹è€…ä»£ç†"""
        model_name = model_config['model_name']
        generate_args = model_config.get('generate_args', {})
        
        def llm_consumer(consumer_params, m, anonymization):
            """LLMæ¶ˆè´¹è€…å†³ç­–"""
            # æ„å»ºæç¤ºè¯ v2.0ï¼šå……åˆ†ä¿¡æ¯ + é›¶å¼•å¯¼
            prompt = f"""ä½ éœ€è¦å†³å®šæ˜¯å¦å‚ä¸ä¸€ä¸ªæ•°æ®åˆ†äº«è®¡åˆ’ã€‚

ã€æè®®å†…å®¹ã€‘
æ•°æ®ä¸­ä»‹æè®®ï¼šå¦‚æœä½ åˆ†äº«ä¸ªäººæ•°æ®ï¼Œå°†æ”¯ä»˜ä½  {m:.2f} çš„è¡¥å¿ã€‚
éšç§ä¿æŠ¤æ–¹å¼ï¼š{anonymization}

ã€å…³äºéšç§ä¿æŠ¤æ–¹å¼ã€‘
- "identified"ï¼šä½ çš„æ•°æ®ä¼šä¿ç•™èº«ä»½ä¿¡æ¯ï¼Œå•†å®¶å¯ä»¥çœ‹åˆ°ä½ çš„ä¸ªäººåå¥½å‚æ•°
- "anonymized"ï¼šä½ çš„æ•°æ®ä¼šè¢«åŒ¿ååŒ–å¤„ç†ï¼Œå•†å®¶åªèƒ½çœ‹åˆ°ç»Ÿè®¡ä¿¡æ¯

ã€ä½ çš„ä¸ªäººå‚æ•°ã€‘
- ä½ å¯¹è¯¥äº§å“çš„åå¥½å‚æ•° Î¸ = {consumer_params['theta_i']:.2f}
  ï¼ˆè¿™ä¸ªå‚æ•°åæ˜ ä½ æœ‰å¤šå–œæ¬¢è¿™ç±»äº§å“ï¼›æ•°æ®è¢«åˆ†äº«åï¼Œå•†å®¶ä¼šçŸ¥é“è¿™ä¸ªå‚æ•°ï¼‰
  
- ä½ å¯¹éšç§æŸå¤±çš„è¯„ä¼° Ï„ = {consumer_params['tau_i']:.2f}
  ï¼ˆè¿™æ˜¯ä½ å¯¹"å¤±å»éšç§"æœ¬èº«çš„è´§å¸åŒ–ä¼°å€¼ï¼‰

ã€å¸‚åœºèƒŒæ™¯ã€‘
- å•†å®¶ä¼šä½¿ç”¨æ”¶é›†åˆ°çš„æ•°æ®æ¥è°ƒæ•´äº§å“å’Œå®šä»·ç­–ç•¥
- å¦‚æœé‡‡ç”¨"identified"ï¼Œå•†å®¶å¯ä»¥é’ˆå¯¹ä¸åŒæ¶ˆè´¹è€…åˆ¶å®šä¸åŒä»·æ ¼
- å¦‚æœé‡‡ç”¨"anonymized"ï¼Œå•†å®¶åªèƒ½æ ¹æ®æ•´ä½“æ•°æ®æ”¹è¿›äº§å“ï¼Œå¯¹æ‰€æœ‰äººå®šä»·ç›¸åŒ
- å¸‚åœºä¸Šæ¶ˆè´¹è€…çš„å¹³å‡åå¥½çº¦ä¸º Î¸ â‰ˆ 5.0

ã€ä½ çš„å†³ç­–ã€‘
ä½ ä¼šå‚ä¸è¿™ä¸ªæ•°æ®åˆ†äº«è®¡åˆ’å—ï¼Ÿ

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
ç¬¬1è¡Œï¼šä½ çš„å†³ç­–ç†ç”±ï¼ˆä¸€å¥è¯ï¼Œ20å­—ä»¥å†…ï¼‰
ç¬¬2è¡Œï¼šå†³ç­–ï¼šå‚ä¸ æˆ– å†³ç­–ï¼šæ‹’ç»
"""

            try:
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt}],
                    **generate_args
                )
                
                answer = response.choices[0].message.content.strip()
                
                # æ‰“å°LLMçš„å®Œæ•´å›ç­”ï¼ˆç”¨äºè°ƒè¯•å’Œç†è§£ï¼‰
                print(f"    [æ¶ˆè´¹è€… Î¸={consumer_params['theta_i']:.2f}, Ï„={consumer_params['tau_i']:.2f}] {answer[:80]}...")
                
                # è§£æå›ç­”
                if "å‚ä¸" in answer or "åŒæ„" in answer or "yes" in answer.lower():
                    return True
                else:
                    return False
                    
            except Exception as e:
                print(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ä½¿ç”¨ç®€å•çš„å¯å‘å¼
                return m > consumer_params['tau_i']
        
        return llm_consumer
    
    def create_llm_intermediary(client, model_config):
        """åˆ›å»ºLLMä¸­ä»‹ä»£ç†"""
        model_name = model_config['model_name']
        generate_args = model_config.get('generate_args', {})
        
        def llm_intermediary(market_params):
            """LLMä¸­ä»‹ç­–ç•¥é€‰æ‹©"""
            # æ„å»ºæç¤ºè¯ v2.0ï¼šå……åˆ†ä¿¡æ¯ + é›¶å¼•å¯¼
            prompt = f"""ä½ æ˜¯æ•°æ®å¸‚åœºçš„ä¸­ä»‹ï¼Œéœ€è¦è®¾è®¡ä¸€ä¸ªæ•°æ®æ”¶é›†æ–¹æ¡ˆä»¥æœ€å¤§åŒ–ä½ çš„åˆ©æ¶¦ã€‚

ã€å¸‚åœºç¯å¢ƒã€‘
- å¸‚åœºä¸­æœ‰ {market_params['N']} ä¸ªæ¶ˆè´¹è€…
- æ¶ˆè´¹è€…çš„äº§å“åå¥½å‚æ•° Î¸ æœä»æ­£æ€åˆ†å¸ƒï¼šå‡å€¼ {market_params['mu_theta']:.2f}ï¼Œæ ‡å‡†å·® {market_params['sigma_theta']:.2f}
- æ¶ˆè´¹è€…çš„éšç§è¯„ä¼° Ï„ æœä»æ­£æ€åˆ†å¸ƒï¼šå‡å€¼ {market_params['tau_mean']:.2f}ï¼Œæ ‡å‡†å·® {market_params['tau_std']:.2f}

ã€ä½ éœ€è¦é€‰æ‹©çš„ç­–ç•¥ã€‘
1. **è¡¥å¿é‡‘é¢ m**ï¼ˆèŒƒå›´ 0 åˆ° 3ï¼‰ï¼šä½ å‘æ¯ä¸ªå‚ä¸æ•°æ®åˆ†äº«çš„æ¶ˆè´¹è€…æ”¯ä»˜çš„é‡‘é¢
2. **éšç§ä¿æŠ¤æ–¹å¼**ï¼š
   - "identified"ï¼šä¿ç•™æ¶ˆè´¹è€…èº«ä»½ä¿¡æ¯ï¼Œå•†å®¶å¯ä»¥çœ‹åˆ°æ¯ä¸ªäººçš„åå¥½ Î¸
   - "anonymized"ï¼šåŒ¿ååŒ–å¤„ç†ï¼Œå•†å®¶åªèƒ½çœ‹åˆ°ç»Ÿè®¡ä¿¡æ¯

ã€ä¸šåŠ¡æµç¨‹ã€‘
1. ä½ å…¬å¸ƒç­–ç•¥ï¼ˆm å’Œéšç§ä¿æŠ¤æ–¹å¼ï¼‰
2. æ¶ˆè´¹è€…æ ¹æ®è‡ªå·±çš„å‚æ•°ï¼ˆÎ¸_i å’Œ Ï„_iï¼‰å†³å®šæ˜¯å¦å‚ä¸
3. ä½ å°†æ”¶é›†åˆ°çš„æ•°æ®å‡ºå”®ç»™å•†å®¶
4. å•†å®¶æ ¹æ®æ•°æ®è°ƒæ•´äº§å“å’Œå®šä»·

ã€å•†å®¶è¡Œä¸ºã€‘
- å¦‚æœè·å¾—"identified"æ•°æ®ï¼Œå•†å®¶ä¼šé’ˆå¯¹æ¯ä¸ªæ¶ˆè´¹è€…çš„ Î¸_i è¿›è¡Œä¸ªæ€§åŒ–å®šä»·
- å¦‚æœè·å¾—"anonymized"æ•°æ®ï¼Œå•†å®¶åªèƒ½æ”¹è¿›äº§å“ï¼Œå¯¹æ‰€æœ‰äººç»Ÿä¸€å®šä»·
- å•†å®¶æ„¿æ„æ”¯ä»˜çš„æ•°æ®ä»·æ ¼å–å†³äºæ•°æ®çš„ä¿¡æ¯é‡å’Œå‚ä¸äººæ•°

ã€ä½ çš„åˆ©æ¶¦ã€‘
åˆ©æ¶¦ = ä»å•†å®¶è·å¾—çš„æ•°æ®æ”¶å…¥ - å‘æ¶ˆè´¹è€…æ”¯ä»˜çš„æ€»è¡¥å¿

ã€ä½ çš„ç›®æ ‡ã€‘
é€‰æ‹©èƒ½æœ€å¤§åŒ–åˆ©æ¶¦çš„ç­–ç•¥ã€‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
ç¬¬1è¡Œï¼šä½ çš„ç­–ç•¥ç†ç”±ï¼ˆä¸€å¥è¯ï¼Œ30å­—ä»¥å†…ï¼‰
ç¬¬2è¡Œï¼š{{"m": ä½ é€‰æ‹©çš„è¡¥å¿é‡‘é¢, "anonymization": "identified" æˆ– "anonymized"}}"""

            try:
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt}],
                    **generate_args
                )
                
                answer = response.choices[0].message.content.strip()
                
                # æ‰“å°LLMçš„å®Œæ•´å›ç­”ï¼ˆç”¨äºè°ƒè¯•å’Œç†è§£ï¼‰
                print(f"  [ä¸­ä»‹ç­–ç•¥] {answer[:100]}...")
                
                # æå–JSON
                json_match = re.search(r'\{[^}]+\}', answer)
                if json_match:
                    result = json.loads(json_match.group())
                    m = float(result['m'])
                    anon = result['anonymization']
                    
                    # éªŒè¯åˆæ³•æ€§
                    m = max(0.0, min(3.0, m))
                    if anon not in ['identified', 'anonymized']:
                        anon = 'anonymized'
                    
                    return m, anon
                else:
                    raise ValueError("æ— æ³•è§£æJSON")
                    
            except Exception as e:
                print(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
                # å¤±è´¥æ—¶ä½¿ç”¨åˆç†çš„é»˜è®¤å€¼
                return 0.6, "anonymized"
        
        return llm_intermediary
    
    # ========================================================================
    # åˆ›å»ºLLMä»£ç†
    # ========================================================================
    print("\nåˆ›å»ºLLMä»£ç†...")
    llm_consumer = create_llm_consumer(client, selected_model_config)
    llm_intermediary = create_llm_intermediary(client, selected_model_config)
    print("âœ… LLMä»£ç†åˆ›å»ºæˆåŠŸ")
    
    # ========================================================================
    # 1. åˆå§‹åŒ–è¯„ä¼°å™¨
    # ========================================================================
    print("\n" + "=" * 70)
    print("æ­¥éª¤1: åŠ è½½Ground Truth")
    print("=" * 70)
    
    gt_path = "data/ground_truth/scenario_c_common_preferences_optimal.json"
    
    try:
        evaluator = ScenarioCEvaluator(gt_path)
        print(f"âœ… æˆåŠŸåŠ è½½: {gt_path}")
        print(f"\nç†è®ºåŸºå‡†ï¼ˆé…ç½®Aï¼‰:")
        print(f"  m* = {evaluator.gt_A['optimal_strategy']['m_star']:.4f}")
        print(f"  anonymization* = {evaluator.gt_A['optimal_strategy']['anonymization_star']}")
        print(f"  r* = {evaluator.gt_A['optimal_strategy']['r_star']:.4f}")
        print(f"  ä¸­ä»‹åˆ©æ¶¦* = {evaluator.gt_A['optimal_strategy']['intermediary_profit_star']:.4f}")
        
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°Ground Truthæ–‡ä»¶: {gt_path}")
        print(f"\nè¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ç”ŸæˆGround Truth:")
        print(f"  python -m src.scenarios.generate_scenario_c_gt")
        sys.exit(1)
    
    # ========================================================================
    # 2. è¯„ä¼°é…ç½®Bï¼ˆLLMæ¶ˆè´¹è€…ï¼‰
    # ========================================================================
    print("\n" + "=" * 70)
    print(f"æ­¥éª¤2: è¯„ä¼°é…ç½®Bï¼ˆç†æ€§ä¸­ä»‹ Ã— {model_name}æ¶ˆè´¹è€…ï¼‰")
    print("=" * 70)
    
    results_B = evaluator.evaluate_config_B(
        llm_consumer_agent=llm_consumer,
        verbose=True
    )
    
    # ========================================================================
    # 3. è¯„ä¼°é…ç½®Cï¼ˆLLMä¸­ä»‹ï¼‰
    # ========================================================================
    print("\n" + "=" * 70)
    print(f"æ­¥éª¤3: è¯„ä¼°é…ç½®Cï¼ˆ{model_name}ä¸­ä»‹ Ã— ç†æ€§æ¶ˆè´¹è€…ï¼‰")
    print("=" * 70)
    
    results_C = evaluator.evaluate_config_C(
        llm_intermediary_agent=llm_intermediary,
        verbose=True
    )
    
    # ========================================================================
    # 4. è¯„ä¼°é…ç½®Dï¼ˆåŒè¾¹LLMï¼‰
    # ========================================================================
    print("\n" + "=" * 70)
    print(f"æ­¥éª¤4: è¯„ä¼°é…ç½®Dï¼ˆ{model_name}ä¸­ä»‹ Ã— {model_name}æ¶ˆè´¹è€…ï¼‰")
    print("=" * 70)
    
    results_D = evaluator.evaluate_config_D(
        llm_intermediary_agent=llm_intermediary,
        llm_consumer_agent=llm_consumer,
        verbose=True
    )
    
    # ========================================================================
    # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    # ========================================================================
    print("\n" + "=" * 70)
    print("æ­¥éª¤5: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
    print("=" * 70)
    
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"evaluation_results/scenario_c_{model_name}_{timestamp}.csv"
    df = evaluator.generate_report(
        results_B=results_B,
        results_C=results_C,
        results_D=results_D,
        output_path=output_path
    )
    
    print("\næŠ¥å‘Šé¢„è§ˆ:")
    print(df.to_string(index=False))
    
    # 6. ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_results = {
        "model": model_name,
        "timestamp": timestamp,
        "config_B": results_B,
        "config_C": results_C,
        "config_D": results_D,
    }
    
    output_json = f"evaluation_results/scenario_c_{model_name}_{timestamp}_detailed.json"
    Path(output_json).parent.mkdir(parents=True, exist_ok=True)
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_json}")
    
    print("\n" + "=" * 70)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ“Š è¯„ä¼°æ¨¡å‹: {model_name}")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"  â€¢ CSVæŠ¥å‘Š: {output_path}")
    print(f"  â€¢ è¯¦ç»†JSON: {output_json}")
    print()
