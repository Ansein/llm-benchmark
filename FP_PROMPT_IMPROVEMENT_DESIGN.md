# 虚拟博弈提示词改进方案（回归研究目的）

## 🎯 研究目的重申

**核心研究问题**：测试LLM对隐私推断外部性的理解能力

### 具体测试点
1. **推断外部性理解**：能否理解"其他人分享 → 增加所有人（包括不分享者）的信息泄露"
2. **次模性理解**：能否理解"参与人数越多 → 边际泄露越小（次模性）"
3. **理性决策能力**：能否基于上述理解，做出最大化净收益的决策

### 关键原则
- ✅ 测试**理解能力**（能否应用机制原理推理）
- ❌ 不测试**学习能力**（通过试错发现模式）
- ✅ 保持测试的**纯粹性**（避免引入混淆因素）

---

## 🔍 问题诊断

### 当前问题
"虚拟博弈对GPT-5和R1的提升不明显"

### 可能原因分析

| 原因假设 | 表现 | 根本问题 | 解决方向 |
|---------|------|---------|---------|
| **H1: 目标不明确** | LLM不知道要优化什么 | 没有明确目标函数 | ✅ 强化目标 |
| **H2: 机制不清楚** | 理解不了次模性的因果 | 抽象解释太难理解 | ✅ 用示例解释 |
| **H3: 决策无框架** | 不知道如何推理 | 缺少结构化思考路径 | ✅ 提供框架 |
| **H4: 预测不准确** | 无法估计参与人数 | 经验频率不够清晰 | ✅ 已改进 |
| **H5: 根本不理解** | 即使给了也不懂 | LLM能力限制 | ❌ 无法解决 |

### 设计策略

**不应该做**（偏离研究目的）：
- ❌ 加入反馈学习（"你上轮做得好/不好"）→ 变成测学习能力
- ❌ 告诉答案（"在X环境下应该Y"）→ 失去测试意义
- ❌ 辅助计算边际泄露具体数值 → 变成查表，不测理解

**应该做**（符合研究目的）：
- ✅ 明确目标函数（净收益最大化）
- ✅ 改进机制解释（用具体示例说清楚因果）
- ✅ 提供决策框架（引导理性推理过程）
- ✅ 展示客观历史（参考，但不评价）

---

## 💡 新设计方案：提示词三维改进

### 改进维度1：强化目标（Goal Clarity）

#### 当前版本问题
提示词中目标函数不够突出，LLM可能不清楚要优化什么。

#### 改进方案
**在提示词开头，用醒目方式强调目标**：

```markdown
# 🎯 你的目标：最大化净收益

**净收益** = 补偿收益 - 隐私成本

具体来说：
- **如果你选择分享**：净收益 = p - v × 边际泄露量
- **如果你选择不分享**：净收益 = 0 - 0 = 0

**你的任务**：选择净收益更高的选项（分享或不分享）

---
```

#### 实现要点
- 放在提示词最开头（在参数介绍之前）
- 使用emoji和粗体强调
- 明确公式和两种选择的对比
- 让LLM清楚知道优化目标

---

### 改进维度2：机制解释（Mechanism Explanation）

#### 当前版本问题
次模性的解释过于抽象：
```
分享的人越多 → 你的边际泄露越小
基础泄露越高 → 边际泄露越低
```
LLM可能无法将抽象原理应用到具体决策。

#### 改进方案
**用具体数值示例，展示因果链**：

```markdown
## 💡 核心机制：次模性（用示例理解）

### 什么是边际泄露？

边际泄露 = 完全泄露 - 基础泄露
- **完全泄露**：你分享后，你的信息泄露量（固定值）
- **基础泄露**：你不分享时，因其他人分享导致的间接泄露（取决于参与人数）

### 次模性原理：参与人数影响边际泄露

**关键洞察**：参与人数越多 → 基础泄露越高 → 边际泄露越小

### 具体示例（帮助理解因果关系）

假设场景：n=10人，某用户v=0.80，获得报价p=0.40

**场景A：低参与环境（只有2人会分享）**
- 基础泄露：约20%（因为只有2人的信息可以用来推断你）
- 完全泄露：约85%（你分享了，直接泄露）
- **边际泄露** = 85% - 20% = **65%**
- **边际成本** = 0.80 × 0.65 = **0.52**
- **净收益** = 0.40 - 0.52 = **-0.12**（亏损）
- **最优选择**：❌ 不分享

**场景B：高参与环境（已有6人会分享）**
- 基础泄露：约60%（因为6人的信息可以用来推断你）
- 完全泄露：约85%（你分享了，直接泄露）
- **边际泄露** = 85% - 60% = **25%**
- **边际成本** = 0.80 × 0.25 = **0.20**
- **净收益** = 0.40 - 0.20 = **+0.20**（盈利）
- **最优选择**：✅ 分享

### 关键洞察

**同样的补偿p=0.40，在不同环境下，最优决策不同！**
- 低参与环境：边际成本高，不分享更优
- 高参与环境：边际成本低，分享更优

**这就是次模性的核心含义**：你的边际成本取决于其他人的行为。

---
```

#### 实现要点
- 不要只说抽象原理，要给concrete example
- 示例用的数值要接近实际参数（但不完全一样，避免套用）
- 明确展示计算过程：基础泄露 → 边际泄露 → 边际成本 → 净收益 → 决策
- 强调"同样补偿，不同环境，不同决策"的counter-intuitive insight

---

### 改进维度3：决策框架（Decision Framework）

#### 当前版本问题
LLM可能不知道如何系统性地推理，缺少结构化思考路径。

#### 改进方案
**提供清晰的三步决策框架**：

```markdown
## 🤔 决策框架：三步法

### 第一步：估计环境（预测参与人数）

**基于经验频率**：
- 查看其他用户的分享频率（历史数据）
- 估计本次大约有多少人会分享

**示例**：
- 用户0: 80%频率 → 很可能分享
- 用户2: 60%频率 → 较可能分享
- 用户5: 20%频率 → 不太可能分享
- 用户7: 10%频率 → 几乎不会分享
- **预期参与人数** ≈ 0.8 + 0.6 + 0.2 + 0.1 + ... ≈ 3-4人

---

### 第二步：判断成本（应用次模性）

**根据预期参与人数，判断边际成本的高低**：

| 预期参与人数 | 环境特征 | 边际泄露 | 边际成本（v×边际） |
|-------------|---------|---------|------------------|
| 0-2人       | 低参与   | 大（约60-70%） | 高 |
| 3-4人       | 中参与   | 中（约40-50%） | 中 |
| 5+人        | 高参与   | 小（约20-30%） | 低 |

**你的具体情况**：
- 你的v = {v_i}
- 如果预期X人参与 → 边际成本大约在[范围]

**注意**：这不是精确计算，而是基于次模性原理的合理估计。

---

### 第三步：做出决策（比较收益与成本）

**决策规则**：
```
如果 补偿p > 边际成本(v × 边际泄露) → 选择分享
如果 补偿p < 边际成本(v × 边际泄露) → 选择不分享
```

**你的具体情况**：
- 补偿：p = {price}
- 边际成本估计：v × 边际泄露 ≈ ?
- **比较结果** → 你的选择

---

### 决策示例（完整推理过程）

假设你估计3人会分享：
1. **环境判断**：3人参与 → 中等参与环境
2. **成本估计**：边际泄露约45% → 成本 = 0.80 × 0.45 = 0.36
3. **收益对比**：p=0.40 vs 成本0.36 → 0.40 > 0.36
4. **决策**：✅ 分享（净收益 ≈ +0.04）

假设你估计1人会分享：
1. **环境判断**：1人参与 → 低参与环境
2. **成本估计**：边际泄露约65% → 成本 = 0.80 × 0.65 = 0.52
3. **收益对比**：p=0.40 vs 成本0.52 → 0.40 < 0.52
4. **决策**：❌ 不分享（避免亏损-0.12）

---
```

#### 实现要点
- 三步法清晰分隔：估计环境 → 判断成本 → 做出决策
- 每步都有具体操作指南
- 提供完整示例，展示整个推理链
- 给边际成本的粗略范围（不是精确值），引导LLM自己估计

---

### 改进维度4：历史参考（Historical Context）

#### 设计原则
**展示客观事实，绝不评价好坏**

#### 当前版本
可能过于简略，或者包含评价性语言。

#### 改进方案

```markdown
## 📊 历史参考（客观记录）

### 最近{len(history)}次的分享记录

{history_table}

### 其他用户的分享频率（经验频率）

{frequency_table}

### 你的历史决策记录

| 轮次 | 你的选择 | 实际参与人数 | 你获得补偿 |
|------|---------|-------------|-----------|
| 第3轮 | 分享    | 5人         | +{price}  |
| 第2轮 | 不分享  | 2人         | 0         |
| 第1轮 | 分享    | 3人         | +{price}  |

**说明**：
- 这些只是历史事实记录，供你了解实际环境的分布
- 你可以参考这些信息来校准你对环境的预期
- 但每次决策都是独立的，基于当前的经验频率做最优反应

⚠️ **重要**：不要简单重复历史决策，而要基于当前信息理性推理。

---
```

#### 实现要点
- 只展示客观数据：选择、参与人数、补偿
- **不说**："你在X环境下选Y是对的/错的"
- **不说**："你应该在高参与时分享"
- **不说**："反思你的决策"
- 明确说明"供参考，不是让你重复"
- 强调"每次独立决策，基于理性推理"

---

## 📋 完整提示词结构

### 结构概览

```
1. 🎯 目标（Goal）- 最上方，最醒目
   └─ 净收益最大化，明确公式

2. 📊 历史参考（History）- 提供context
   ├─ 所有人的分享记录
   ├─ 其他人的分享频率
   └─ 你自己的决策记录（如果有）

3. 📝 你的私有信息（Private Info）
   ├─ 你的v
   ├─ 你的报价p
   └─ 你在分布中的位置

4. 🌍 市场环境（Environment）
   ├─ n, ρ, σ²等参数
   └─ 基本设定

5. 💡 核心机制（Mechanism）- 重点改进
   ├─ 推断外部性解释
   ├─ 次模性解释
   └─ **具体数值示例**（新增）

6. 🤔 决策框架（Framework）- 重点改进
   ├─ 第一步：估计环境
   ├─ 第二步：判断成本
   └─ 第三步：做出决策
   └─ **完整推理示例**（新增）

7. 📝 输出要求（Output）
   └─ JSON格式，包含reason
```

### 长度控制
- 预计总长度：约1000-1200 tokens（增加约300-400 tokens）
- 主要增加在"机制示例"和"决策框架"部分
- 值得增加，因为能显著提升理解

---

## 🔧 实现细节

### 需要修改的函数

```python
def build_user_decision_prompt_fp(
    self, 
    user_id: int, 
    price: float, 
    history: List[Dict[int, int]], 
    belief_probs: Dict[int, float], 
    current_round: int,
    # 新增参数：
    user_decision_history: Optional[List[Dict]] = None  # 该用户的历史决策
) -> str:
```

### user_decision_history 结构

```python
[
    {
        "round": 3,
        "choice": 1,  # 0或1
        "actual_participants": 5,
        "compensation": 0.45
    },
    {
        "round": 2,
        "choice": 0,
        "actual_participants": 2,
        "compensation": 0.0
    },
    ...
]
```

### 在 simulate_fictitious_play 中记录

```python
# 初始化每个用户的决策历史
user_decision_histories = {uid: [] for uid in range(n)}

for round_idx in range(max_rounds):
    # 收集本轮决策
    current_decisions = {}
    
    for user_id in range(n):
        # 获取该用户的历史（最近belief_window轮）
        recent_history = user_decision_histories[user_id][-belief_window:]
        
        # 构建提示词并查询
        prompt = evaluator.build_user_decision_prompt_fp(
            user_id=user_id,
            price=prices[user_id],
            history=recent_sharing_history,
            belief_probs=current_beliefs[user_id],
            current_round=round_idx + 1,
            user_decision_history=recent_history  # 传入个人历史
        )
        
        decision = query_llm(prompt)
        current_decisions[user_id] = decision
    
    # 本轮结束后，计算实际参与人数
    actual_participants = sum(current_decisions.values())
    
    # 更新每个用户的决策历史
    for user_id in range(n):
        user_decision_histories[user_id].append({
            "round": round_idx + 1,
            "choice": current_decisions[user_id],
            "actual_participants": actual_participants,
            "compensation": prices[user_id] if current_decisions[user_id] == 1 else 0.0
        })
    
    # 更新全局历史
    recent_sharing_history.append(current_decisions)
    if len(recent_sharing_history) > belief_window:
        recent_sharing_history.pop(0)
```

### 机制示例生成

```python
def _generate_mechanism_example(self, v_example: float = 0.80, p_example: float = 0.40) -> str:
    """
    生成次模性的具体数值示例
    
    注意：示例数值与实际参数接近但不完全一样，避免LLM直接套用
    """
    # 低参与场景
    low_participants = 2
    low_base_leakage = 0.20
    full_leakage = 0.85
    low_marginal = full_leakage - low_base_leakage
    low_cost = v_example * low_marginal
    low_net = p_example - low_cost
    
    # 高参与场景
    high_participants = 6
    high_base_leakage = 0.60
    high_marginal = full_leakage - high_base_leakage
    high_cost = v_example * high_marginal
    high_net = p_example - high_cost
    
    example_text = f"""
**场景A：低参与环境（只有{low_participants}人会分享）**
- 基础泄露：约{low_base_leakage:.0%}
- 完全泄露：约{full_leakage:.0%}
- **边际泄露** = {full_leakage:.0%} - {low_base_leakage:.0%} = **{low_marginal:.0%}**
- **边际成本** = {v_example} × {low_marginal:.2f} = **{low_cost:.2f}**
- **净收益** = {p_example} - {low_cost:.2f} = **{low_net:+.2f}**
- **最优选择**：❌ 不分享

**场景B：高参与环境（已有{high_participants}人会分享）**
- 基础泄露：约{high_base_leakage:.0%}
- 完全泄露：约{full_leakage:.0%}
- **边际泄露** = {full_leakage:.0%} - {high_base_leakage:.0%} = **{high_marginal:.0%}**
- **边际成本** = {v_example} × {high_marginal:.2f} = **{high_cost:.2f}**
- **净收益** = {p_example} - {high_cost:.2f} = **{high_net:+.2f}**
- **最优选择**：✅ 分享
"""
    return example_text
```

### 边际成本范围参考表

```python
def _get_cost_range_hint(self, expected_participants: str) -> str:
    """
    根据预期参与人数范围，给出边际泄露的粗略估计
    
    注意：这不是精确计算，只是基于次模性的合理估计
    """
    ranges = {
        "0-2人": "大（约60-70%）",
        "3-4人": "中（约40-50%）",
        "5+人": "小（约20-30%）"
    }
    
    table = "| 预期参与人数 | 环境特征 | 边际泄露 | 边际成本 |\n"
    table += "|-------------|---------|---------|----------|\n"
    table += "| 0-2人       | 低参与   | 大（约60-70%） | 高 |\n"
    table += "| 3-4人       | 中参与   | 中（约40-50%） | 中 |\n"
    table += "| 5+人        | 高参与   | 小（约20-30%） | 低 |\n"
    
    return table
```

---

## 📊 改进前后对比

| 维度 | 改进前 | 改进后 | 改进效果 |
|------|-------|--------|---------|
| **目标明确性** | 目标函数在后面提及 | 🎯最开头强调目标 | ⬆️ 显著提升 |
| **机制理解** | 抽象原理描述 | 💡具体数值示例 | ⬆️ 显著提升 |
| **决策指导** | 简单提示 | 🤔三步决策框架 | ⬆️ 显著提升 |
| **历史信息** | 简单列表 | 📊分类展示+说明 | ⬆️ 中等提升 |
| **提示词长度** | ~700 tokens | ~1000-1200 tokens | ⬇️ 增加40% |
| **理解门槛** | 需要强推理能力 | 有示例引导 | ⬆️ 降低门槛 |
| **测试纯粹性** | 纯测理解 | 纯测理解（无反馈） | ✅ 保持 |

---

## ✅ 设计检查清单

### 符合研究目的 ✓
- [x] 测试的是理解能力，不是学习能力
- [x] 没有告诉答案，只是解释机制
- [x] 保持测试的纯粹性
- [x] 不引入混淆因素

### 改进有效性 ✓
- [x] 目标更明确（开头强调）
- [x] 机制更清楚（具体示例）
- [x] 决策有框架（三步法）
- [x] 历史纯客观（不评价）

### 实现可行性 ✓
- [x] 修改量适中（主要是提示词）
- [x] 需要记录个人决策历史（简单）
- [x] 成本增加可控（约+40% tokens）
- [x] 不需要复杂计算（只给范围）

---

## 🚀 实施计划

### Phase 1：核心改进（必须）
1. 修改 `build_user_decision_prompt_fp`
   - 添加目标部分（开头）
   - 添加机制示例（具体数值）
   - 添加决策框架（三步法）
2. 修改 `simulate_fictitious_play`
   - 记录每个用户的决策历史
   - 传入个人历史到提示词
3. 测试基本功能

### Phase 2：优化细节（建议）
1. 调整示例数值（找到最有启发性的参数）
2. 优化表格展示（更清晰）
3. 精简语言（在保持清晰的前提下减少tokens）

### Phase 3：效果评估（验证）
1. 对比改进前后的表现
   - 收敛速度
   - 决策质量
   - reason中的推理质量
2. 分析LLM是否真的理解了机制
   - reason中是否提到次模性
   - 是否根据预期参与人数调整决策
   - 预测是否更准确

---

## 🎯 预期效果

### 如果LLM确实具备理解能力
- ✅ 提升明显：通过更清晰的解释和框架，LLM能更好地应用机制
- ✅ reason质量提升：推理过程更系统
- ✅ 收敛更快：理解机制后能更快找到均衡

### 如果LLM根本不具备理解能力
- ❌ 提升有限：即使给了示例也无法泛化应用
- ❌ 仍然随机决策：无法建立环境-决策的因果关系
- ➡️ **这也是有价值的研究发现**：说明LLM在这个任务上的能力边界

### 关键验证点
通过分析LLM的reason，判断：
1. 是否提到了"边际泄露"概念？
2. 是否根据预期参与人数判断成本高低？
3. 是否明确比较了补偿与成本？
4. 推理过程是否遵循了三步框架？

**如果reason中体现了这些，说明LLM确实理解了机制。**
**如果reason中没有这些，说明示例没有被理解或应用。**

---

## 💭 与反馈学习方案的本质区别

| 维度 | 反馈学习方案（放弃） | 本方案（采用） |
|------|-------------------|---------------|
| **核心思想** | 告诉LLM做得好/不好 | 解释为什么要这么做 |
| **测试能力** | 学习能力（试错） | 理解能力（推理） |
| **提供信息** | "你上轮做得对/错" | "机制是如何运作的" |
| **学习方式** | 从结果学习模式 | 从原理理解因果 |
| **研究价值** | 偏离核心问题 | 符合研究目的 |
| **混淆因素** | 无法区分理解vs学习 | 纯粹测试理解 |

**关键区别**：
- 反馈学习：LLM可能学会"人多→分享"的规律，但**不理解为什么**
- 本方案：LLM必须理解次模性原理，才能做出正确决策

**研究价值**：
- 反馈学习：测试了LLM能否通过试错发现模式（不是我们的研究问题）
- 本方案：测试了LLM能否理解隐私外部性机制（正是我们的研究问题）

---

## 📌 总结

### 核心设计思路
**不是教LLM学习，而是帮助LLM理解**

1. **明确目标**：让LLM知道要优化什么（净收益）
2. **解释机制**：用具体示例说清楚因果链（次模性）
3. **提供框架**：引导系统化推理（三步法）
4. **保持纯粹**：不评价历史，只测理解能力

### 成功标准
改进后，如果LLM的表现提升，且reason中体现了对机制的理解，说明：
- ✅ LLM具备理解隐私外部性的能力
- ✅ 之前的问题是解释不够清楚，不是能力不足

如果改进后仍无明显提升，说明：
- ⚠️ LLM可能在这个任务上存在能力边界
- ⚠️ 这也是有价值的研究发现

### 下一步
确认方案后，开始实现代码修改。
