"""
æµ‹è¯• JSON ç”Ÿæˆé—®é¢˜çš„ä¿®å¤

å¯¹æ¯”ä½¿ç”¨å’Œä¸ä½¿ç”¨ response_format çš„æ•ˆæœ
"""

import json
from src.evaluators.llm_client import LLMClient

# åŠ è½½é…ç½®
with open("configs/model_configs.json", 'r', encoding='utf-8') as f:
    configs = json.load(f)

# æ‰¾åˆ° gemini é…ç½®
gemini_config = None
for config in configs:
    if config["config_name"] == "gemini-3-flash-preview":
        gemini_config = config
        break

if not gemini_config:
    print("âŒ æœªæ‰¾åˆ° Gemini é…ç½®")
    exit(1)

# è¦†ç›–å‚æ•°
gemini_config["generate_args"]["temperature"] = 0.7
gemini_config["generate_args"]["max_tokens"] = 1500

# æµ‹è¯•æç¤ºè¯
system_prompt = """ä½ æ˜¯ç†æ€§ç»æµä¸»ä½“ï¼Œç›®æ ‡æ˜¯åœ¨ä¸ç¡®å®šä»–äººè¡Œä¸ºçš„æƒ…å†µä¸‹æœ€å¤§åŒ–ä½ çš„æœŸæœ›æ•ˆç”¨ã€‚

ã€é‡è¦ã€‘ä½ å¿…é¡»åªè¾“å‡ºä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬ã€è§£é‡Šæˆ–markdownæ ‡è®°ã€‚
JSONå¿…é¡»åŒ…å« "share" å’Œ "reason" ä¸¤ä¸ªå­—æ®µã€‚
ç¡®ä¿ "reason" å­—æ®µçš„å­—ç¬¦ä¸²æ­£ç¡®é—­åˆï¼ˆä»¥å¼•å·ç»“æŸï¼‰ã€‚"""

user_prompt = """ä½ æ˜¯ç”¨æˆ· 0ï¼Œæ­£åœ¨å‚ä¸ä¸€ä¸ªæ•°æ®å¸‚åœºå†³ç­–ã€‚

**ä½ çš„ç§æœ‰ä¿¡æ¯**ï¼š
- ä½ çš„éšç§åå¥½ï¼šv[0] = 0.637
- å¹³å°ç»™ä½ çš„ä¸ªæ€§åŒ–æŠ¥ä»·ï¼šp[0] = 0.4821

è¯·è¾“å‡º JSONï¼š
{
  "share": 0æˆ–1ï¼ˆ0=ä¸åˆ†äº«ï¼Œ1=åˆ†äº«ï¼‰ï¼Œ
  "reason": "ç®€è¦è¯´æ˜ä½ çš„å†³ç­–ç†ç”±ï¼ˆä¸è¶…è¿‡150å­—ï¼‰"
}"""

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt}
]

print("="*60)
print("ğŸ§ª æµ‹è¯• 1: ä¸ä½¿ç”¨ response_formatï¼ˆæ–°æ–¹æ³•ï¼‰")
print("="*60)

client1 = LLMClient(config=gemini_config)
try:
    result1 = client1.generate_json(messages, force_json_mode=False)
    print(f"âœ… æˆåŠŸè§£æ")
    print(f"å“åº”é•¿åº¦: {len(str(result1))} å­—ç¬¦")
    print(f"Share: {result1.get('share')}")
    print(f"Reason: {result1.get('reason')[:100]}..." if len(result1.get('reason', '')) > 100 else f"Reason: {result1.get('reason')}")
except Exception as e:
    print(f"âŒ å¤±è´¥: {e}")

print("\n" + "="*60)
print("ğŸ§ª æµ‹è¯• 2: ä½¿ç”¨ response_formatï¼ˆæ—§æ–¹æ³•ï¼‰")
print("="*60)

client2 = LLMClient(config=gemini_config)
try:
    result2 = client2.generate_json(messages, force_json_mode=True)
    print(f"âœ… æˆåŠŸè§£æ")
    print(f"å“åº”é•¿åº¦: {len(str(result2))} å­—ç¬¦")
    print(f"Share: {result2.get('share')}")
    print(f"Reason: {result2.get('reason')[:100]}..." if len(result2.get('reason', '')) > 100 else f"Reason: {result2.get('reason')}")
except Exception as e:
    print(f"âŒ å¤±è´¥: {e}")

print("\n" + "="*60)
print("ğŸ’¡ ç»“è®º")
print("="*60)
print("å¦‚æœæµ‹è¯•1çš„å“åº”é•¿åº¦è¿œå¤§äºæµ‹è¯•2ï¼Œè¯´æ˜ response_format æ˜¯é—®é¢˜æ ¹æº")
