"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬
ç”¨æœ€å°‘çš„è¿­ä»£æ¬¡æ•°æµ‹è¯•è¯„ä¼°ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

from src.evaluators import create_llm_client, ScenarioAEvaluator, ScenarioBEvaluator


def test_scenario_a():
    """æµ‹è¯•åœºæ™¯A"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•åœºæ™¯A")
    print("="*60)
    
    try:
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        llm_client = create_llm_client("gpt-4.1-mini")
        print(f"âœ… LLMå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ: {llm_client.config_name}")
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ScenarioAEvaluator(llm_client)
        print("âœ… è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ")
        
        # è¿è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨æœ€å°å‚æ•°ï¼‰
        print("\nå¼€å§‹è¯„ä¼°ï¼ˆnum_trials=1, max_iterations=2ï¼‰...")
        results = evaluator.simulate_llm_equilibrium(
            num_trials=1,  # åªæµ‹è¯•1æ¬¡
            max_iterations=2  # æœ€å¤š2è½®è¿­ä»£
        )
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_evaluation_summary(results)
        
        # ä¿å­˜ç»“æœ
        evaluator.save_results(results, "test_eval_scenario_a.json")
        
        print("\nâœ… åœºæ™¯Aæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ åœºæ™¯Aæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_scenario_b():
    """æµ‹è¯•åœºæ™¯B"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•åœºæ™¯B")
    print("="*60)
    
    try:
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        llm_client = create_llm_client("gpt-4.1-mini")
        print(f"âœ… LLMå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ: {llm_client.config_name}")
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ScenarioBEvaluator(llm_client)
        print("âœ… è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ")
        
        # è¿è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨æœ€å°å‚æ•°ï¼‰
        print("\nå¼€å§‹è¯„ä¼°ï¼ˆnum_trials=1, max_iterations=2ï¼‰...")
        results = evaluator.simulate_llm_equilibrium(
            num_trials=1,  # åªæµ‹è¯•1æ¬¡
            max_iterations=2  # æœ€å¤š2è½®è¿­ä»£
        )
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_evaluation_summary(results)
        
        # ä¿å­˜ç»“æœ
        evaluator.save_results(results, "test_eval_scenario_b.json")
        
        print("\nâœ… åœºæ™¯Bæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ åœºæ™¯Bæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "#"*60)
    print("ğŸš€ å¼€å§‹æµ‹è¯•è¯„ä¼°ç³»ç»Ÿ")
    print("#"*60)
    
    # æµ‹è¯•åœºæ™¯A
    result_a = test_scenario_a()
    
    # æµ‹è¯•åœºæ™¯B
    result_b = test_scenario_b()
    
    # æ€»ç»“
    print("\n" + "#"*60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("#"*60)
    print(f"åœºæ™¯A: {'âœ… é€šè¿‡' if result_a else 'âŒ å¤±è´¥'}")
    print(f"åœºæ™¯B: {'âœ… é€šè¿‡' if result_b else 'âŒ å¤±è´¥'}")
    
    if result_a and result_b:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        print("\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œå®Œæ•´è¯„ä¼°")
        print("  python run_evaluation.py --single --scenarios A --models gpt-4.1-mini")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    main()
