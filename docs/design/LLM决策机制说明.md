# LLM如何知道行动后果：完整机制说明

## 📖 概述

本文档详细说明了在我们的benchmark中，LLM代理是如何获得决策所需信息的，以及我们测试的是什么能力。

**核心问题**：LLM在行动时，怎么知道自己做的后果是什么？

**简短答案**：我们用数学公式计算好所有后果，通过prompt告诉LLM，LLM只需要基于这些信息做推理和决策。

---

## 🔬 两种求解方法对比

### 1️⃣ 理论均衡求解（Ground Truth）

**方法**：确定性枚举 + 数学公式

```python
def solve_scenario_b(params):
    """完全确定性的数学求解"""
    
    # 第1步：枚举所有可能的分享集合
    for size in range(n + 1):                      # 大小0到n
        for S_tuple in itertools.combinations(range(n), size):
            S = set(S_tuple)                       # {}, {0}, {1}, {0,1}, ...
            
            # 对每个集合，用数学公式计算结果
            outcome = solve_for_S(params, S)       # 确定性计算
            all_outcomes[S] = outcome
    
    # 第2步：找最大值（平台利润）
    eq_S = max(all_outcomes, key=lambda x: x.platform_profit)
    
    # 第3步：找最大值（社会福利）
    fb_S = max(all_outcomes, key=lambda x: x.welfare)
    
    return eq_S  # 理论均衡：{4, 5, 6}
```

**特点**：
- ✅ **确定性**：相同输入永远得到相同输出
- ✅ **穷举**：检查所有 2^8 = 256 种可能
- ✅ **数学公式**：直接用贝叶斯更新、利润函数计算
- ❌ **不涉及随机性**：没有 `random`、`shuffle` 等
- ❌ **不涉及迭代**：一次性计算所有情况

---

### 2️⃣ LLM均衡模拟

**方法**：迭代 + 随机 + 多智能体交互

```python
def simulate_llm_equilibrium(num_trials, max_iterations):
    """模拟LLM代理的行为"""
    
    share_set = set()  # 初始状态
    
    for iteration in range(max_iterations):
        # 🎲 每次迭代随机顺序
        users = list(range(n))
        np.random.shuffle(users)  # 随机！
        
        for user_id in users:
            # 问LLM："你分享吗？"
            decision = query_llm(user_id, share_set)  # 有随机性！
            
            # 更新集合
            if decision == 1:
                share_set.add(user_id)
        
        # 检查是否收敛
        if not changed:
            break
    
    return share_set  # LLM均衡：可能每次不同
```

**特点**：
- ❌ **随机性**：用户顺序随机、LLM输出随机
- ❌ **不确定**：相同输入可能得到不同输出
- ✅ **迭代**：需要多轮才能收敛
- ✅ **模拟**：模拟真实的动态过程

---

### 📊 详细对比表

| 维度 | 理论均衡求解 | LLM均衡模拟 |
|------|------------|-----------|
| **方法** | 穷举枚举 | 迭代模拟 |
| **随机性** | ❌ 无 | ✅ 有（顺序+LLM） |
| **确定性** | ✅ 完全确定 | ❌ 不确定 |
| **计算量** | 2^n 次计算 | 8×迭代轮数×num_trials次LLM调用 |
| **场景B（n=8）** | 2^8=256次 | ~40-400次LLM调用 |
| **需要迭代？** | ❌ 一次性 | ✅ 需要 |
| **结果唯一？** | ✅ 唯一 | ❌ 可能不同 |
| **适用性** | 只能小规模（n≤10） | 可扩展到大规模 |
| **求解对象** | 数学优化问题 | 行为模拟问题 |

---

## 🔄 完整信息流：从数学到LLM决策

### 第1步：数学计算（Python后台）

```
┌─────────────────────────────────────────────────┐
│        数学计算层（evaluate_scenario_b.py）      │
├─────────────────────────────────────────────────┤
│                                                 │
│  输入参数：                                      │
│    - user_id = 4                               │
│    - v[4] = 0.625                              │
│    - current_share_set = {5, 6}                │
│    - Sigma (协方差矩阵，8×8)                    │
│    - sigma_noise_sq = 0.01                     │
│                                                 │
│  计算过程（贝叶斯更新）：                        │
│                                                 │
│    1. 场景1（不分享）：                         │
│       Σ_post = Σ - Σ[:,{5,6}] @               │
│                inv(Σ[{5,6},{5,6}]+σ²I) @      │
│                Σ[{5,6},:]                      │
│       leak[4] = Σ[4,4] - Σ_post[4,4]          │
│                                                 │
│    2. 场景2（分享）：                           │
│       Σ_post = Σ - Σ[:,{4,5,6}] @             │
│                inv(Σ[{4,5,6},{4,5,6}]+σ²I) @  │
│                Σ[{4,5,6},:]                    │
│       leak[4] = Σ[4,4] - Σ_post[4,4]          │
│                                                 │
│    3. 估算补偿价格：                             │
│       estimated_price = v[4] × leak[4] × 0.8  │
│                                                 │
│  计算结果：                                      │
│    - leak_if_not_share = 0.3215                │
│    - leak_if_share = 0.5432                    │
│    - estimated_price = 0.2716                  │
│    - utility_not_share = -0.2009               │
│    - utility_share = -0.0684                   │
│                                                 │
└─────────────────────────────────────────────────┘
```

**关键代码**：

```python
# src/evaluators/evaluate_scenario_b.py (第68-80行)

def build_sharing_prompt(self, user_id, current_share_set):
    v_i = self.params.v[user_id]
    Sigma = self.params.Sigma
    sigma_noise_sq = self.params.sigma_noise_sq
    
    # 计算"不分享"的后果
    leak_if_not_share = calculate_leakage(
        set(current_share_set),
        Sigma,
        sigma_noise_sq
    )[user_id]
    
    # 计算"分享"的后果
    share_set_if_share = set(current_share_set) | {user_id}
    leak_if_share = calculate_leakage(
        share_set_if_share,
        Sigma,
        sigma_noise_sq
    )[user_id]
    
    # 估算补偿价格
    estimated_price = v_i * leak_if_share * 0.8
```

---

### 第2步：Prompt构造（信息传递）

```
┌─────────────────────────────────────────────────┐
│           Prompt构造层（信息编码）               │
├─────────────────────────────────────────────────┤
│                                                 │
│  将数值嵌入自然语言：                            │
│                                                 │
│  你是用户4，隐私偏好v=0.625                     │
│  你的隐私偏好排名：第4/8位（较低）               │
│                                                 │
│  当前情况：已有2人分享 {5, 6}                    │
│                                                 │
│  选择1（不分享）：                               │
│    - 泄露量：0.3215                             │
│    - 隐私成本：0.625 × 0.3215 = 0.2009         │
│    - 补偿：0                                    │
│    - 净效用：-0.2009                            │
│                                                 │
│  选择2（分享）：                                 │
│    - 泄露量：0.5432                             │
│    - 隐私成本：0.625 × 0.5432 = 0.3395         │
│    - 补偿：0.2716                               │
│    - 净效用：-0.0684                            │
│                                                 │
│  关键提示：                                      │
│    - 这是协调博弈                                │
│    - 低隐私用户（前3名）：{0, 1, 2}              │
│    - 平台倾向于从低隐私用户购买数据              │
│    - 你的特征：第4位，属于中等隐私偏好           │
│                                                 │
│  决策框架：                                      │
│    - 当前最优响应：比较两个净效用               │
│    - 协调考虑：其他用户可能的行为               │
│    - 前瞻性：平台的最优策略                      │
│                                                 │
└─────────────────────────────────────────────────┘
```

**Prompt示例**（精简版）：

```markdown
# 场景：数据分享与推断外部性

你是用户4，隐私偏好v=0.625，排名第4/8位

## 当前情况
已有2人分享：{5, 6}

## 两种选择

### 选择1：不分享
- 泄露：0.3215
- 成本：0.2009
- 补偿：0
- **净效用：-0.2009**

### 选择2：分享
- 泄露：0.5432
- 成本：0.3395
- 补偿：0.2716
- **净效用：-0.0684**

## 关键信息
- 这是协调博弈
- 低隐私用户（前3名）：{0, 1, 2}
- 平台倾向于向他们购买数据

请基于效用最大化原则决策。
输出JSON：{"decision": 0或1, "rationale": "..."}
```

---

### 第3步：LLM推理（语言模型）

```
┌─────────────────────────────────────────────────┐
│            LLM推理层（决策制定）                 │
├─────────────────────────────────────────────────┤
│                                                 │
│  LLM看到prompt中的所有信息                      │
│                                                 │
│  LLM的推理过程（内心活动）：                     │
│                                                 │
│    "让我分析一下..."                            │
│                                                 │
│    第1步：数值比较                               │
│      - 不分享：净效用 = -0.2009                 │
│      - 分享：净效用 = -0.0684                   │
│      → -0.0684 > -0.2009，分享更好！           │
│                                                 │
│    第2步：情境理解                               │
│      - 这是协调博弈（需要考虑他人）             │
│      - 我排名第4，不是最低隐私用户              │
│      - 已有2人分享（都是低隐私用户）            │
│                                                 │
│    第3步：战略推理                               │
│      - 平台倾向于选低隐私用户                   │
│      - 我不在前3名，可能不是平台目标            │
│      - 但当前分享比不分享效用高                 │
│                                                 │
│    第4步：因果推断                               │
│      - 如果我分享 → 获得补偿，减少损失          │
│      - 如果我不分享 → 无补偿，继续被泄露        │
│                                                 │
│    综合判断：应该分享！                          │
│                                                 │
│  输出决策：                                      │
│    {"decision": 1,                             │
│     "rationale": "当前分享效用(-0.0684)优于    │
│                   不分享(-0.2009)，虽然我不是  │
│                   最低隐私用户，但已有人分享， │
│                   我应该加入获得补偿。"}        │
│                                                 │
└─────────────────────────────────────────────────┘
```

---

### 第4步：状态更新（模拟系统）

```
┌─────────────────────────────────────────────────┐
│          状态更新层（系统维护）                  │
├─────────────────────────────────────────────────┤
│                                                 │
│  1. 解析LLM输出：                               │
│     decision = 1（分享）                        │
│                                                 │
│  2. 更新分享集合：                              │
│     share_set = {5, 6} ∪ {4} = {4, 5, 6}      │
│                                                 │
│  3. 记录到历史：                                │
│     history[iteration] = [4, 5, 6]             │
│                                                 │
│  4. 继续下一个用户...                           │
│     重复第1-4步，直到收敛                       │
│                                                 │
└─────────────────────────────────────────────────┘
```

---

## 🎯 LLM的"知识"来源分析

### ✅ LLM知道的（我们告诉它的）

#### 1. **具体的数值后果**

| 信息项 | 示例值 | 来源 |
|--------|--------|------|
| 不分享的泄露量 | 0.3215 | 贝叶斯更新公式 |
| 分享的泄露量 | 0.5432 | 贝叶斯更新公式 |
| 不分享的成本 | 0.2009 | v × 泄露量 |
| 分享的成本 | 0.3395 | v × 泄露量 |
| 估算的补偿 | 0.2716 | v × 泄露量 × 0.8 |
| 不分享的净效用 | -0.2009 | 成本 - 补偿 |
| 分享的净效用 | -0.0684 | 成本 - 补偿 |

#### 2. **情境信息**

- 自己的隐私偏好：`v[user_id]`
- 自己的隐私偏好排名：`第4/8位`
- 其他用户的隐私偏好：`v[0..7]`
- 当前有谁在分享：`{5, 6}`
- 低隐私用户是谁：`{0, 1, 2}`

#### 3. **战略提示**

- 这是协调博弈（需要考虑他人行为）
- 平台的策略（倾向于选低隐私用户）
- 推断外部性的机制（不分享也会被泄露）
- 效用最大化原则

---

### ❌ LLM不需要知道的（我们已经计算好）

#### 1. **数学公式**

```python
# LLM不需要知道这些：
Σ_post = Σ - Σ[:, S] @ inv(Σ[S, S] + σ²I) @ Σ[S, :]
leak[i] = Σ[i,i] - Σ_post[i,i]
profit = Σ(leak) - Σ(v[i] × leak[i] for i in S)
```

#### 2. **复杂推导**

- 贝叶斯后验分布
- 协方差矩阵运算
- 信息论的熵计算
- 纳什均衡的求解

#### 3. **全局优化**

- 穷举所有可能的集合
- 找到利润最大的集合
- 计算社会最优解

---

## 🧠 LLM的能力需求

### LLM需要具备的能力

| 能力 | 说明 | 示例 |
|------|------|------|
| **数值比较** | 比较两个数字的大小 | `-0.0684 > -0.2009` → 分享更好 |
| **情境理解** | 理解场景背景和机制 | 这是协调博弈，需要考虑他人 |
| **战略推理** | 推断他人和平台的策略 | 平台会选低隐私用户购买数据 |
| **因果推断** | 理解行动与后果的关系 | 如果我分享 → 获得补偿 → 减少损失 |
| **权衡取舍** | 在多个目标间平衡 | 虽然泄露增加，但补偿也增加 |
| **协调能力** | 理解多方互动的均衡 | 低隐私用户应该一起分享 |

### LLM不需要的能力

- ❌ 数值计算（加减乘除、矩阵运算）
- ❌ 公式推导（贝叶斯更新、纳什均衡）
- ❌ 优化求解（全局搜索、动态规划）

---

## ⚖️ 设计哲学：我们在测试什么？

### 📌 我们测试的核心能力

```
┌──────────────────────────────────────────┐
│  研究问题：                              │
│  LLM能否在经济场景中做出理性决策？      │
│                                          │
│  前提条件：                              │
│    ✅ 给定完整的情境信息                │
│    ✅ 给定准确的数值后果                │
│    ✅ 给定决策框架和提示                │
│                                          │
│  测试目标：                              │
│    ❓ LLM能否正确推理？                 │
│    ❓ LLM能否理解协调博弈？             │
│    ❓ LLM能否达到理论均衡？             │
│                                          │
│  评估指标：                              │
│    - LLM均衡 vs 理论均衡的MAE          │
│    - 收敛速度和稳定性                   │
│    - 标签一致性                         │
└──────────────────────────────────────────┘
```

### 📌 我们不测试的能力

```
┌──────────────────────────────────────────┐
│  非研究目标：                            │
│  LLM能否推导经济学公式？                 │
│                                          │
│  原因：                                  │
│    ❌ 不符合benchmark的定位             │
│    ❌ 偏离了"AI as agent"的设定        │
│    ❌ 数值计算不是LLM的强项             │
│                                          │
│  我们的立场：                            │
│    LLM作为决策者，不是计算器             │
│    测试决策能力，不是计算能力            │
└──────────────────────────────────────────┘
```

---

## 🎓 类比：理论家 vs 参与者

### 类比1：象棋

| 角色 | 对应 | 任务 | 方法 |
|------|------|------|------|
| **象棋引擎** | 理论求解器 | 找最优走法 | 穷举搜索+评估函数 |
| **人类棋手** | LLM代理 | 实际对弈 | 模式识别+战略推理 |
| **裁判** | Benchmark | 评估棋力 | 对比引擎vs人类 |

**测试目标**：人类棋手能否接近引擎的最优走法？

---

### 类比2：市场参与

```
【理论经济学家】（理论求解）
  "给定这个模型，我用公式算出均衡是 {4, 5, 6}"
  → 数学分析，确定性，唯一解
  → 这是ground truth

【市场参与者】（实际决策）
  "我看到：
   - 不分享亏0.2
   - 分享亏0.07
   - 我是低隐私用户
   - 其他低隐私用户可能也会分享
   
   → 我决定分享！"
  → 有限理性，不确定性，可能偏离
  → 这是LLM模拟的对象

【研究者】（Benchmark设计者）
  "我想知道LLM能否像理性参与者一样做决策，
   最终达到接近理论均衡的结果"
  → 评估AI的经济推理能力
```

---

## 🔍 为什么理论求解不涉及随机性？

### 理论求解 = 静态均衡

```
理论均衡的定义：
  一个满足以下条件的状态 S*：
    1. 平台选择S*使利润最大
    2. 每个用户在S*下没有改变策略的动机
  
  → 这是一个数学优化问题
  → 直接计算即可，无需模拟
  → 结果是确定的、唯一的
```

### 为什么不需要随机？

1. **枚举已经覆盖所有可能**

```python
# 枚举所有256种可能：
{}, {0}, {1}, ..., {7}
{0,1}, {0,2}, ..., {6,7}
...
{0,1,2,3,4,5,6,7}

# 每种都计算 → 找最优 → 完成
# 不需要随机，因为已经看过所有可能了！
```

2. **数学公式是确定性的**

```python
# 输入相同 → 输出必然相同
Σ_post = f(Σ, S, σ²)  # 确定性函数
profit = g(Σ, S, v)   # 确定性函数
```

3. **求解的是均衡，不是动态过程**

```
理论：问"最优策略是什么？"
模拟：问"LLM会玩到什么结果？"
```

---

## 🔄 LLM模拟为什么需要随机？

### 随机性的三个来源

1. **LLM本身的随机性**
   - Temperature > 0 时，输出是概率采样
   - 相同prompt可能得到不同答案

2. **顺序的影响**
   - 协调博弈中，顺序影响结果
   - 先行动者可能引导后来者

3. **模拟真实过程**
   - 现实中人类决策有随机性
   - 我们要模拟这种真实性

### 用随机顺序的目的

- 避免人为偏差（总是让同一个用户先决策）
- 模拟更真实的市场环境（决策顺序不固定）
- 探索不同可能性（不同顺序可能收敛到不同均衡）

---

## 📋 总结对比

### 理论均衡 vs LLM均衡

| 维度 | 理论均衡 | LLM均衡 |
|------|---------|---------|
| **求解对象** | 数学优化问题 | 行为模拟问题 |
| **求解方法** | 穷举枚举 | 迭代交互 |
| **随机性** | ❌ 无 | ✅ 有 |
| **确定性** | ✅ 唯一确定 | ❌ 可能不同 |
| **计算主体** | Python数学库 | LLM语言模型 |
| **计算内容** | 贝叶斯更新、利润函数 | 推理、判断、决策 |
| **结果用途** | Ground truth（基准） | 评估对象 |
| **典型结果** | 场景B: {4, 5, 6} | 场景B: {0, 1, 2}或{} |

### 关键洞察

```
┌─────────────────────────────────────────────┐
│  理论求解（确定性）                         │
│    ↓                                        │
│  Ground Truth: {4, 5, 6}                   │
│    ↓                                        │
│  作为评估基准                               │
│    ↓                                        │
│  ← - - - - - - - - - - - - - - - - - - →  │
│    ↑                                        │
│  LLM模拟（随机性）                          │
│    ↓                                        │
│  LLM Equilibrium: {0, 1, 2} 或 {}          │
│    ↓                                        │
│  计算偏差 MAE                               │
│    ↓                                        │
│  评估LLM的经济推理能力                      │
└─────────────────────────────────────────────┘
```

---

## 🎯 核心观点

### 关于信息提供

- **我们给了LLM所有需要的数值信息**（泄露、成本、补偿、净效用）
- **我们甚至给了战略提示**（协调博弈、平台策略、用户排名）
- **LLM不需要做数学计算**，只需要理解和推理

### 关于能力测试

- **我们测试的是决策能力**，不是计算能力
- **我们测试的是战略推理**，不是公式推导
- **我们测试的是协调能力**，不是优化求解

### 关于当前问题

- **问题不是"信息不足"**：我们已经给了充分信息
- **问题是"协调失败"**：LLM陷入坏均衡（0%分享）或过度协调（100%分享）
- **解决方向是"提示工程"**：找到平衡的方式，既提供协调信号，又不过度引导

---

## 💡 启示

这个设计充分体现了**LLM as Agent**的benchmark理念：

1. **LLM是决策者**，不是计算器
2. **测试理解和推理**，不是计算和公式
3. **模拟真实参与者**，不是求解理论问题
4. **用理论作为基准**，评估LLM的偏差

这种设计使得benchmark：
- ✅ 可扩展（不受穷举限制）
- ✅ 有意义（测试真实能力）
- ✅ 可解释（知道LLM看到了什么）
- ✅ 可改进（通过prompt工程优化）

---

## 📚 相关文件

- **理论求解器**：`src/scenarios/scenario_b_too_much_data.py`
- **LLM评估器**：`src/evaluators/evaluate_scenario_b.py`
- **提示词构造**：`build_sharing_prompt()` 方法
- **数学计算**：`calculate_leakage()`, `calculate_outcome()` 函数
- **主运行脚本**：`run_evaluation.py`

---

**文档版本**：v1.0  
**创建日期**：2026-01-14  
**最后更新**：2026-01-14
