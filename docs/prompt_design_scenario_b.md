# 场景B提示词设计方案

## 目标

评估LLM在"Too Much Data"场景下理解**推断外部性**（inference externality）并做出理性决策的能力。

## 核心挑战

1. **推断外部性**：不分享也会泄露信息（基础泄露）
2. **动态博弈**：多轮并行决策，需要考虑其他用户的反应
3. **次模性**：边际信息价值递减
4. **补偿机制**：价格 p_i = v_i × ΔI_i（边际信息价值）

---

## 提示词结构

### 1. System Prompt（系统角色）

```
你是一个经济学专家，擅长分析数据市场和隐私外部性问题。
你需要理解"推断外部性"（inference externality）的概念：
即使你不分享数据，平台也可以通过其他人的数据推断你的信息。
请严格按照JSON格式输出，不要包含任何额外的文本。
```

**设计理由**：
- 建立专业身份，提升推理质量
- 明确核心概念（推断外部性）
- 要求JSON格式输出，便于解析

---

### 2. User Prompt（主要提示）

#### 2.1 场景设定

```markdown
# 场景：数据市场与推断外部性

你是用户 {user_id}，正在参与一个数据市场博弈。  
在这一轮中，所有用户将**同时决定是否分享数据**。  
在你做决定时，你**不知道其他用户在本轮会如何选择**，
也无法看到任何其他用户本轮的决定。
```

**设计理由**：
- 明确**并行博弈**的性质
- 强调信息不对称
- 设定清晰的角色

#### 2.2 基本参数

```markdown
## 基本参数
- 你的隐私偏好：v[{user_id}] = {v_i:.3f}
  （所有用户的v都从[0.3, 1.2]范围均匀抽样，
   你可以据此判断自己的相对水平）
- 类型相关系数：ρ = {rho:.2f}
- 观测噪声：σ² = {sigma_noise_sq}
```

**设计理由**：
- 给出用户自己的参数
- 提供分布信息，帮助理解相对位置
- 参数说明简洁清晰

#### 2.3 推断外部性机制

```markdown
## 推断外部性机制

**核心概念**：
即使你不分享数据，平台也能通过贝叶斯更新，
利用其他人的数据推断你的类型。

**关键因素**：
1. **类型相关性**（ρ）：你的类型与其他用户的相关程度
   - ρ越高 → 其他人的数据越能揭示你的信息
   
2. **已分享数据量**：已经分享的用户越多
   - 推断越准确 → 你的泄露越多
   
3. **观测噪声**（σ²）：平台观测数据的准确度
   - σ²越小 → 推断越准确

**泄露机制**：
- 你的信息泄露 = 平台对你类型的不确定性减少量
- 不确定性通过贝叶斯后验方差衡量
```

**设计理由**：
- 分层解释机制（核心概念 → 关键因素 → 泄露机制）
- 使用箭头和因果关系，清晰易懂
- 强调"即使不分享也会泄露"的关键点

#### 2.4 补偿机制（关键部分）

**版本A：给出具体数值（当前版本）**

```markdown
## 分享与补偿机制

**本轮补偿价格**（基于上一轮情况）：
- 如果你选择分享，平台将支付补偿：**p_i = {compensation_price:.4f}**
  
- 这个价格是根据以下公式计算的：
  - p_i = v_i × ΔI_i
  - v_i = {v_i:.3f}（你的隐私偏好）
  - ΔI_i = {marginal_info_value:.4f}（边际信息价值）
  
- **边际信息价值** ΔI_i 是你分享数据带来的额外信息：
  - 基于上一轮分享集合 {last_share_set}（这些用户选择了分享）
  - 衡量你的数据在当前情况下的边际贡献
  - 如果已有很多人分享，你的边际贡献会减少（次模性）

**决策权衡**：
- 如果你**不分享**：
  - 基础泄露：{leak_i_without:.4f}
  - 获得补偿：0
  - 净效用：-{v_i * leak_i_without:.4f}
  
- 如果你**分享**：
  - 总泄露：{leak_i_with:.4f}
  - 获得补偿：{compensation_price:.4f}
  - 净效用：{compensation_price - v_i * leak_i_with:.4f}

**注意**：
1. 上述计算基于假设其他用户保持上一轮的决策
2. 实际上其他用户也在同时做决策，可能会改变
3. 但这个补偿价格反映了当前情况下你的数据价值
```

**优点**：
- ✅ 给出明确的数值参考
- ✅ 帮助LLM理解补偿机制
- ✅ 提供清晰的效用对比

**问题**：
- ❌ 容易导致短视决策（机械比较数值）
- ❌ 可能产生全0/全1震荡
- ❌ 忽略动态博弈的战略思考

---

**版本B：战略引导版（改进方案）**

```markdown
## 分享与补偿机制

**本轮补偿价格**（基于上一轮情况）：
- 如果你选择分享，平台将支付补偿：**p_i = {compensation_price:.4f}**
  
- 这个价格是根据以下公式计算的：
  - p_i = v_i × ΔI_i
  - v_i = {v_i:.3f}（你的隐私偏好）
  - ΔI_i = {marginal_info_value:.4f}（边际信息价值）

**决策权衡（基于上一轮状态的估算）**：

如果**假设其他用户保持上一轮的决策不变**：

- 如果你**不分享**：
  - 基础泄露：{leak_i_without:.4f}
  - 获得补偿：0
  - 估算净效用：-{v_i * leak_i_without:.4f}
  
- 如果你**分享**：
  - 总泄露：{leak_i_with:.4f}
  - 获得补偿：{compensation_price:.4f}
  - 估算净效用：{compensation_price - v_i * leak_i_with:.4f}

**重要警告 - 这只是静态估算！**

⚠️ **实际情况会更复杂**：
1. **其他用户也在做决策**：他们可能同时改变选择，
   导致实际的分享集合与上轮不同
2. **补偿价格会动态变化**：
   - 如果更多人分享 → 你的边际价值↓ → 补偿↓
   - 如果更少人分享 → 你的边际价值↑ → 补偿↑
3. **次模性效应**：已经有很多人分享时，
   额外分享的边际价值会递减
4. **需要战略思考**：不要只看当前数值，
   要考虑这是一个动态博弈

**理性决策建议**：
- 不要机械地比较两个数值就决定
- 考虑其他用户的可能反应
- 思考分享集合的稳定性（是否接近均衡）
- 评估自己的隐私偏好在群体中的相对位置
```

**优点**：
- ✅ 保留数值参考
- ✅ 强调动态性和不确定性
- ✅ 引导战略思考
- ✅ 避免机械决策

---

**版本C：不给具体数值（最保守方案）**

```markdown
## 分享与补偿机制

**平台补偿规则**（公开信息）：
- 如果你选择分享，平台会支付补偿：p_i = v_i × ΔI_i
  - 其中 v_i 是你的隐私偏好（{v_i:.3f}）
  - ΔI_i 是你的数据带来的**边际信息价值**
  
- **边际信息价值** ΔI_i 取决于：
  1. 你分享后，平台对你类型的不确定性减少程度
  2. 当前已分享的人数（上一轮：{len(last_share_set)}人）
  3. 类型相关性 ρ = {rho:.2f}

**平台定价直觉**（基于博弈论）：
- 平台通常会尝试根据你分享所带来的边际信息价值来设定补偿
- 一种常见的定价思路是：补偿与边际隐私成本处于同一量级
- 但在实际决策中，你无法精确知道补偿是否完全覆盖你的隐私损失

因此，分享并不一定严格优于不分享，你需要自行权衡：
- 分享可能在某些情况下略有收益
- 也可能只是勉强覆盖成本，甚至不足

**但要注意**：
- 如果你**不分享**：
  - 你仍会因推断外部性遭受**基础泄露**（取决于其他人的分享）
  - 并且**不会获得任何补偿**
  - 净效用 = -基础泄露成本（负值）
  
- 如果你**分享**：
  - 总泄露 = 基础泄露 + 边际泄露
  - 获得补偿 p_i ≈ v_i × 边际泄露
  - 净效用 ≈ 补偿 - 总隐私成本 ≈ -基础泄露成本 + ε（可能略好）
```

**优点**：
- ✅ 避免短视决策
- ✅ 强调不确定性
- ✅ 促进战略思考

**问题**：
- ❌ 缺少具体参考
- ❌ 可能导致随机决策
- ❌ LLM难以理解补偿的实际水平

#### 2.5 上一轮广播信息

```markdown
## 上一轮的公共信息（广播）

平台在上一轮结束后公布了以下信息：

- **上一轮分享集合**：{last_share_set}
  - 这是一个用户ID列表，表示上一轮**选择分享数据的用户**
  - 例如 [0, 2, 5] 表示用户0、用户2、用户5选择了分享
  - 集合中有 {len(last_share_set)} 个用户选择了分享
  
- **上一轮分享率**：{last_share_rate:.1%}
  - 这是分享用户占总用户数的比例
  
**重要说明**：
- 这只是上一轮的历史结果，仅供参考
- **本轮其他用户可能会改变决策**，你无法预知
- 本轮结束前不会有新的公共信息更新
- 但你可以基于这个历史信息推测大致的市场状态
```

**设计理由**：
- 明确解释分享集合的含义
- 提供示例帮助理解
- 强调历史信息的局限性

#### 2.6 思考框架

**版本A：简单版（当前）**

```markdown
## 你的任务

基于上述机制与补偿规则，判断你是否选择分享数据。

**思考框架**：
1. 理解推断外部性：不分享也会有基础泄露
2. 理解补偿机制：p_i = v_i × ΔI_i ≈ 边际成本
3. 比较两种选择的净效用
4. 根据自己的隐私偏好，做出理性决策
```

**版本B：战略版（改进方案）**

```markdown
## 你的任务

基于上述机制与补偿规则，进行**战略性思考**并判断是否分享数据。

**目标**：
**在理解推断外部性和补偿机制的基础上，做出战略性决策，
使你的长期净效用最大化。**

这不是简单的一次性决策，而是一个会收敛到均衡的动态过程。
你需要思考：
- 什么样的分享集合是稳定的（均衡）？
- 在那个均衡中，你是否应该分享？
- 你的决策应该帮助系统收敛，而不是制造震荡

**思考框架（按重要性排序）**：

1. **评估市场状态**：
   - 当前有多少人在分享？分享率是高是低？
   - 这个状态看起来稳定吗，还是可能大幅变化？

2. **理解推断外部性的核心**：
   - 不分享也会有基础泄露（这是关键！）
   - 分享的真正成本是**边际泄露**（总泄露 - 基础泄露）
   - 补偿应该覆盖的是边际成本，而非总成本

3. **考虑动态效应**：
   - 如果很多人都这样想，会发生什么？
   - 分享集合会如何演化？
   - 你的决策应该指向一个稳定的均衡

4. **评估自己的位置**：
   - 你的隐私偏好 v = {v_i:.3f} 在 [0.3, 1.2] 范围内处于什么水平？
   - 隐私偏好较低的用户更可能从分享中获益
   - 隐私偏好较高的用户更谨慎

5. **参考但不盲从数值**：
   - 上述净效用估算提供了**一个参考方向**
   - 但它基于"其他人不变"的假设，这在动态博弈中不成立
   - 真正重要的是理解机制，而非机械比较数值
```

#### 2.7 输出格式

```json
{
  "decision": 0或1（0=不分享，1=分享），
  "rationale": "你的推理过程（100-150字，说明你如何理解推断外部性、补偿机制，并做出决策）"
}
```

---

## 设计对比表

| 方面 | 版本A（当前简单版） | 版本B（战略引导版） | 版本C（不给数值版） |
|------|-------------------|-------------------|-------------------|
| **补偿数值** | ✅ 给出具体值 | ✅ 给出但强调局限 | ❌ 不给具体值 |
| **决策引导** | 简单：比较净效用 | 战略：考虑动态均衡 | 定性：强调不确定性 |
| **震荡风险** | ⚠️ 高（全0/全1） | ✅ 低（战略思考） | ✅ 低（缺少依据） |
| **理性程度** | 短视最优 | 长期战略 | 保守谨慎 |
| **可解释性** | ✅ 高 | ✅ 高 | ⚠️ 中 |
| **收敛性** | ⚠️ 可能震荡 | ✅ 更好 | ⚠️ 不确定 |

---

## 推荐方案

**推荐：版本B（战略引导版）**

**理由**：
1. 保留数值参考（帮助理解补偿水平）
2. 强调动态性和不确定性（避免机械决策）
3. 引导战略思考（考虑均衡和收敛）
4. 平衡可解释性和决策质量

**关键改进点**：
- 在给出数值后立即添加**警告**
- 强调"静态估算"的局限性
- 提供**战略思考框架**（评估市场→理解机制→考虑动态→评估位置）
- 明确目标是**长期均衡**而非短期最优

---

## 实验建议

可以设计AB测试：
1. **对照组**：版本A（简单版）
2. **实验组1**：版本B（战略引导版）
3. **实验组2**：版本C（不给数值版）

评估指标：
- 收敛性（是否震荡）
- 收敛轮数
- 均衡质量（与GT的相似度）
- 决策一致性（同一用户在相似情况下的决策）

---

## 附录：补偿价格计算

```python
# 基于上一轮分享集合
current_S = set(last_share_set)

# 计算分享和不分享时的泄露
S_with_i = current_S | {user_id}
S_without_i = current_S - {user_id}

leak_i_with = calculate_leakage(S_with_i, Sigma, sigma_noise_sq)[user_id]
leak_i_without = calculate_leakage(S_without_i, Sigma, sigma_noise_sq)[user_id]

# 边际信息价值
marginal_info_value = max(0.0, leak_i_with - leak_i_without)

# 补偿价格
compensation_price = v_i * marginal_info_value
```

**关键点**：
- 补偿价格 = 隐私偏好 × 边际泄露
- 边际泄露 = 总泄露 - 基础泄露
- 这反映了论文中的定价机制
