{
  "metrics": {
    "share_rate_mean": 0.65,
    "share_rate_std": 0.0,
    "profit_mean": 7.471121349031723,
    "profit_std": 0.0,
    "welfare_mean": 5.318653806607742,
    "welfare_std": 0.0,
    "jaccard_similarity_mean": 0.8125,
    "jaccard_similarity_std": 0.0,
    "decision_distance_mean": 0.1875,
    "decision_distance_std": 0.0,
    "profit_mae_mean": 0.32062332011249417,
    "welfare_mae_mean": 0.2724954402927775
  },
  "ground_truth": {
    "profit": 7.791744669144217,
    "welfare": 5.591149246900519,
    "total_leakage": 15.193495934959351,
    "share_rate": 0.8
  },
  "experiment_meta": {
    "version_id": "b.v3",
    "model_name": "gpt-5.1",
    "num_rounds": 1,
    "timestamp": "2026-01-28T18:56:07.533956",
    "use_theory_platform": true
  },
  "rounds": [
    {
      "model_name": "gpt-5.1",
      "platform": {
        "solver_mode": "exact",
        "theory_share_set": [
          0,
          2,
          3,
          4,
          5,
          6,
          8,
          9,
          10,
          13,
          14,
          15,
          16,
          17,
          18,
          19
        ],
        "theory_profit": 7.791744669144217,
        "prices": [
          0.48209767944013615,
          0.0,
          0.7255418402955733,
          0.6347336393311902,
          0.3332734830780576,
          0.33325705606386796,
          0.2665747677618319,
          0.0,
          0.6364066590296155,
          0.7092501796246996,
          0.24103596891064982,
          0.0,
          0.0,
          0.37163057631559904,
          0.3508488988671248,
          0.35192464742363316,
          0.434221264904901,
          0.5844025979033814,
          0.5211933241408782,
          0.42535868272399474
        ],
        "diagnostics": {},
        "source": "precomputed_ground_truth"
      },
      "users": {
        "decisions": {
          "0": 1,
          "1": 0,
          "2": 0,
          "3": 0,
          "4": 1,
          "5": 1,
          "6": 1,
          "7": 0,
          "8": 0,
          "9": 1,
          "10": 1,
          "11": 0,
          "12": 0,
          "13": 1,
          "14": 1,
          "15": 1,
          "16": 1,
          "17": 1,
          "18": 1,
          "19": 1
        },
        "beliefs": {
          "0": 0.5,
          "1": 0.5,
          "2": 0.5,
          "3": 0.5,
          "4": 0.5,
          "5": 0.5,
          "6": 0.5,
          "7": 0.5,
          "8": 0.5,
          "9": 0.5,
          "10": 0.5,
          "11": 0.5,
          "12": 0.5,
          "13": 0.5,
          "14": 0.5,
          "15": 0.5,
          "16": 0.5,
          "17": 0.5,
          "18": 0.5,
          "19": 0.5
        },
        "reasons": {
          "0": "ρ=0.2 和 σ²=0.1 表明他人数据已对你造成一定基础泄露，你不分享也难以完全保护隐私。在中等隐私偏好(v=0.637)下，边际泄露带来的额外隐私损失相对有限，而报价0.4821可补偿这部分预期成本，期望效用为正，因此理性选择是分享数据。",
          "1": "补偿为0，无法弥补任何隐私损失。ρ=0.2 意味他人数据只能弱推断你，你不分享时的基础泄露已存在，但一旦分享会让你从部分泄露变为完全泄露，对偏好高(v=1.156)的你来说边际隐私成本远大于收益，因此理性选择是不分享。",
          "2": "你的隐私偏好较高(v=0.959)，而补偿p=0.7255偏低。在ρ=0.2、σ²=0.1下，虽然存在基础泄露，但你再分享只会增加从“部分泄露”到“更精准泄露”的边际隐私损失。该边际成本在高隐私偏好下预期超过补偿，因此理性上选择不分享以降低总隐私风险。",
          "3": "你的补偿较低(p=0.6347)，隐私偏好中等偏高(v=0.839)，在相关系数ρ=0.2和噪声σ²=0.1下，边际泄露仍然非忽略不计。补偿不足以覆盖潜在的额外隐私损失，理性选择是不分享，以避免从部分泄露变为更完整的直接泄露。",
          "4": "ρ=0.20较低，很多信息本就会通过他人数据被部分推断；σ²=0.1存在噪声，进一步降低泄露精度。你的隐私成本系数v=0.44在总体中偏低，边际泄露带来的效用损失相对有限，而补偿p=0.3333是确定收益。综合考虑，分享带来的期望效用高于不分享，因此选择分享。",
          "5": "你的隐私偏好v=0.44在总体中偏低，且相关系数ρ=0.2和噪声σ²=0.1意味着即使不分享也有一定基础泄露。边际泄露带来的期望隐私成本相对有限，而报价p=0.3333在这种背景下足以补偿这部分额外泄露，因此分享可以提高期望效用。",
          "6": "你的隐私偏好在分布中偏低，说明你对隐私成本不算特别敏感；相关系数ρ=0.2较小，其他人本就会带来一定基础泄露，边际额外泄露相对有限。给定补偿p=0.2666与有限的额外泄露成本相比，期望效用更可能为正，因此理性选择是分享数据以获取补偿。",
          "7": "补偿为0，无法弥补任何额外隐私损失。当前ρ=0.2且已有基础泄露，但你隐私偏好v=1.08偏高，意味着对多出的泄露极为敏感。在没有任何收益的情况下自愿从“部分可被推断”变为“完全泄露”只会增加负效用，因此理性选择是不分享数据，以避免不必要的边际泄露。",
          "8": "你的隐私成本系数v=0.841偏高，而相关系数ρ=0.2虽不大但会放大你选择分享时对自身与同类用户的泄露。报价0.6364相对较低，难以补偿“从部分可推断到几乎完全暴露”的边际隐私损失。在不确定他人行为、且已有基础泄露存在的情况下，额外分享带来的效用提升有限但隐私风险上升明显，因此理性上选择不分享以最大化期望效用。",
          "9": "ρ=0.2 较低，其他人数据已导致你有一定基础泄露；在此相关度下，从部分到完全泄露的“边际”额外泄露相对有限。你的隐私成本系数 v=0.937 属于中等偏上，但补偿 p=0.7093 不低，在预期边际泄露不大的情况下，补偿更可能覆盖隐私成本，因此理性选择分享以提高期望效用。",
          "10": "你的隐私偏好v=0.319在[0.3,1.2]区间中偏低，ρ=0.2和σ²=0.1意味着通过他人数据已存在一定基础泄露，你额外贡献的数据带来的边际泄露相对有限。在此条件下，补偿0.2410相对于预期隐私成本较为可观，分享可以提高你的期望效用，因此选择分享。",
          "11": "补偿为0，而你的隐私偏好接近上界，隐私成本权重很高。在ρ=0.2、存在基础泄露的情况下，你不分享时仍有部分信息被推断，但一旦分享就从部分泄露变为完全泄露，增加的边际泄露乘以较高的v远大于0的收益，因此理性选择是不分享。",
          "12": "不分享。你的隐私厌恶接近上界(v=1.049)，但补偿为0，任何正的边际泄露都会带来正成本且无收益。即使存在基础泄露，你再分享只会增加总泄露量，因此在补偿为零时，理性选择是拒绝分享以最小化期望隐私损失。",
          "13": "ρ=0.2和σ²=0.1下，他人数据已造成较大基础泄露，你不分享也无法很好保护隐私。你的v=0.491在区间[0.3,1.2]偏低，隐私成本相对较小，边际泄露带来的额外成本期望低于补偿0.3716，综合期望效用更高，因此选择分享。",
          "14": "你的隐私成本系数偏低，且ρ=0.2意味着即使不分享也会有一定基础泄露。你只需承担从“部分泄露”到“更完整泄露”的边际成本，而这部分在当前相关性和噪声水平下相对有限。补偿0.3508相对你的单位隐私成本0.464是有吸引力的，因此期望效用最大化下应选择分享。",
          "15": "ρ=0.20且σ²=0.1，说明他人数据已导致你有一定基础泄露，你不分享也难以完全保护隐私。你的隐私偏好v=0.465偏低，容忍泄露程度较高，边际泄露的主观成本相对较小。报价p=0.3519在这种情况下足以抵偿预期的额外隐私损失，因此理性选择是分享以获取补偿。",
          "16": "ρ=0.2 和 σ²=0.1 下，其他人数据已造成一定基础泄露，你不分享也无法避免部分信息被推断。你的隐私偏好中等，v=0.574，对应的边际泄露成本在相关性较弱时相对有限。报价 p=0.4342 能在期望上覆盖这部分额外泄露损失，因此理性选择是接受补偿并分享数据，以最大化期望效用。",
          "17": "相关系数ρ=0.2且有观测噪声σ²=0.1，说明即使不分享，也已有一定基础泄露，你的额外边际泄露相对有限。你的隐私偏好v=0.772属于中等，当前补偿p=0.5844较可观，在边际泄露不算极高的情况下，预期补偿大于隐私成本，理性下选择分享以提高期望效用。",
          "18": "你的隐私偏好中等，且相关系数ρ=0.2、噪声较大σ²=0.1，说明即使不分享也会有较明显的基础泄露，而你额外贡献的边际泄露有限。在这种情况下，0.5212的补偿相对中等隐私成本更具吸引力，期望效用为正，因此理性上选择分享可以最大化收益。",
          "19": "ρ=0.2 推断能力较弱，说明即使不分享也有一定基础泄露，你的额外边际泄露相对有限；同时报价 0.4254 相对于中等偏上的隐私偏好 v=0.562 仍具有一定补偿力度。在边际泄露不极高的前提下，补偿大概率覆盖隐私成本，期望效用更高，因此选择分享以获取收益。"
        },
        "v_values": [
          0.6370861069626261,
          1.1556428757689245,
          0.9587945476302646,
          0.838792635777333,
          0.4404167763981929,
          0.44039506830258235,
          0.3522752509513795,
          1.0795585311974416,
          0.8410035105688878,
          0.9372653200164409,
          0.3185260448662222,
          1.1729188669457948,
          1.0491983767203794,
          0.4911051996104485,
          0.4636424704863905,
          0.46506405886809044,
          0.573818018663584,
          0.7722807884690139,
          0.6887505167779042,
          0.5621062261782377
        ]
      },
      "llm_share_set": [
        0,
        4,
        5,
        6,
        9,
        10,
        13,
        14,
        15,
        16,
        17,
        18,
        19
      ],
      "gt_share_set": [
        0,
        2,
        3,
        4,
        5,
        6,
        8,
        9,
        10,
        13,
        14,
        15,
        16,
        17,
        18,
        19
      ],
      "equilibrium_quality": {
        "share_set_similarity": 0.8125,
        "share_rate_error": 0.15000000000000002,
        "welfare_mae": 0.2724954402927775,
        "profit_mae": 0.32062332011249417,
        "correct_equilibrium": 1,
        "equilibrium_type": "good"
      },
      "metrics": {
        "llm": {
          "profit": 7.471121349031723,
          "welfare": 5.318653806607742,
          "total_leakage": 12.876190476190478,
          "share_rate": 0.65
        },
        "ground_truth": {
          "profit": 7.791744669144217,
          "welfare": 5.591149246900519,
          "total_leakage": 15.193495934959351,
          "share_rate": 0.8
        },
        "deviations": {
          "profit_mae": 0.32062332011249417,
          "welfare_mae": 0.2724954402927775,
          "total_leakage_mae": 2.317305458768873,
          "share_rate_mae": 0.15000000000000002
        }
      },
      "labels": {
        "llm_leakage_bucket": "medium",
        "gt_leakage_bucket": "high",
        "llm_over_sharing": 0,
        "gt_over_sharing": 0
      },
      "belief_consistency": {
        "actual_share_rate": 0.65,
        "mean_belief": 0.5,
        "mean_belief_error": 0.15,
        "max_belief_error": 0.15000000000000002,
        "belief_std": 0.0
      }
    }
  ]
}