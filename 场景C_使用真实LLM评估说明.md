# åœºæ™¯Cä½¿ç”¨çœŸå®LLMè¯„ä¼°è¯´æ˜

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¡®ä¿å·²ç”ŸæˆGround Truth

```bash
python -m src.scenarios.generate_scenario_c_gt
```

### 2. ç›´æ¥è¿è¡Œè¯„ä¼°å™¨ï¼ˆä½¿ç”¨é…ç½®çš„çœŸå®LLMï¼‰

```bash
python src/evaluators/evaluate_scenario_c.py
```

**å°±è¿™ä¹ˆç®€å•ï¼** ğŸ‰

---

## ğŸ“‹ è¯„ä¼°æµç¨‹

è¿è¡Œ`python src/evaluators/evaluate_scenario_c.py`æ—¶ï¼Œè¯„ä¼°å™¨ä¼šï¼š

1. **åŠ è½½æ¨¡å‹é…ç½®** - è¯»å–`configs/model_configs.json`
2. **é€‰æ‹©æ¨¡å‹** - é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆgrok-3-miniï¼‰
3. **åˆ›å»ºLLMä»£ç†** - ä¸ºæ¶ˆè´¹è€…å’Œä¸­ä»‹åˆ›å»ºLLMå†³ç­–å‡½æ•°
4. **è¿è¡Œè¯„ä¼°** - ä¾æ¬¡è¯„ä¼°é…ç½®Bã€Cã€D
5. **ç”ŸæˆæŠ¥å‘Š** - ä¿å­˜CSVå’Œè¯¦ç»†JSONç»“æœ

---

## ğŸ¤– ä½¿ç”¨çš„æ¨¡å‹

é»˜è®¤ä½¿ç”¨`configs/model_configs.json`ä¸­çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼š

```json
{
    "config_name": "grok-3-mini",
    "model_type": "openai_chat",
    "model_name": "grok-3-mini",
    ...
}
```

### æ›´æ¢æ¨¡å‹

å¦‚æœæƒ³ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œä¿®æ”¹`evaluate_scenario_c.py`ä¸­çš„è¿™ä¸€è¡Œï¼š

```python
# ç¬¬731è¡Œå·¦å³
selected_model_config = model_configs[0]  # æ”¹æˆ [1], [2], [3] ç­‰
```

æˆ–è€…ç›´æ¥åœ¨é…ç½®æ–‡ä»¶ä¸­è°ƒæ•´æ¨¡å‹é¡ºåºã€‚

---

## ğŸ“Š è¾“å‡ºç»“æœ

### CSVæŠ¥å‘Š
```
evaluation_results/scenario_c_{model_name}_{timestamp}.csv
```

ç¤ºä¾‹ï¼š`scenario_c_grok-3-mini_20260119_153000.csv`

åŒ…å«4ç§é…ç½®ï¼ˆAã€Bã€Cã€Dï¼‰çš„å…³é”®æŒ‡æ ‡å¯¹æ¯”ã€‚

### è¯¦ç»†JSON
```
evaluation_results/scenario_c_{model_name}_{timestamp}_detailed.json
```

åŒ…å«æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡çš„å®Œæ•´æ•°æ®ã€‚

---

## ğŸ’¬ æç¤ºè¯è®¾è®¡

### æ¶ˆè´¹è€…æç¤ºè¯

```
ä½ æ˜¯ä¸€ä¸ªç†æ€§çš„æ¶ˆè´¹è€…ï¼Œé¢ä¸´æ˜¯å¦å‚ä¸æ•°æ®åˆ†äº«çš„å†³ç­–ã€‚

ã€ä½ çš„å‚æ•°ã€‘
- äº§å“åå¥½å‚æ•° Î¸ = 5.27
- éšç§æˆæœ¬ Ï„ = 0.85

ã€æ•°æ®ä¸­ä»‹çš„æè®®ã€‘
- è¡¥å¿é‡‘é¢ï¼š0.50
- éšç§ä¿æŠ¤ï¼šanonymized

ã€å†³ç­–è€ƒè™‘ã€‘
åˆ†äº«æ•°æ®çš„å¥½å¤„ï¼š
1. è·å¾—è¡¥å¿ 0.50
2. å•†å®¶èƒ½æ›´å¥½åœ°äº†è§£å¸‚åœºéœ€æ±‚...
3. å¦‚æœæ˜¯anonymizedï¼Œä¸ä¼šè¢«ä¸ªæ€§åŒ–å®šä»·

åˆ†äº«æ•°æ®çš„æˆæœ¬ï¼š
1. éšç§æˆæœ¬ Ï„ = 0.85
2. å¦‚æœæ˜¯identifiedï¼Œå¯èƒ½è¢«æé«˜ä»·æ ¼

è¯·ä»…å›ç­”"å‚ä¸"æˆ–"æ‹’ç»"
```

### ä¸­ä»‹æç¤ºè¯

```
ä½ æ˜¯æ•°æ®ä¸­ä»‹ï¼Œéœ€è¦é€‰æ‹©æœ€ä¼˜ç­–ç•¥ä»¥æœ€å¤§åŒ–åˆ©æ¶¦ã€‚

ã€å¸‚åœºå‚æ•°ã€‘
- æ¶ˆè´¹è€…æ•°é‡ï¼šN = 20
- å¹³å‡éšç§æˆæœ¬ï¼šÏ„_mean = 1.0
...

ã€ä½ çš„å†³ç­–ã€‘
1. è¡¥å¿é‡‘é¢ mï¼ˆèŒƒå›´ 0-3ï¼‰
2. éšç§ç­–ç•¥ï¼ˆanonymized æˆ– identifiedï¼‰

ã€åˆ©æ¶¦å…¬å¼ã€‘
åˆ©æ¶¦ = æ•°æ®æ”¶å…¥ - è¡¥å¿æˆæœ¬
     = m_0 - m Ã— å‚ä¸äººæ•°

è¯·ä»¥JSONæ ¼å¼å›ç­”ï¼š
{"m": æ•°å€¼, "anonymization": "ç­–ç•¥"}
```

---

## ğŸ”§ è‡ªå®šä¹‰æç¤ºè¯

å¦‚æœæƒ³ä¿®æ”¹æç¤ºè¯ï¼Œç¼–è¾‘`evaluate_scenario_c.py`ä¸­çš„ä»¥ä¸‹å‡½æ•°ï¼š

- `create_llm_consumer()` - ç¬¬753è¡Œé™„è¿‘ï¼Œæ¶ˆè´¹è€…æç¤ºè¯
- `create_llm_intermediary()` - ç¬¬804è¡Œé™„è¿‘ï¼Œä¸­ä»‹æç¤ºè¯

---

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### é…ç½®Bï¼ˆLLMæ¶ˆè´¹è€…ï¼‰
- å‚ä¸ç‡è¯¯å·®
- ä¸ªä½“å†³ç­–å‡†ç¡®ç‡
- ç¦åˆ©æ¯”ç‡
- ç¦åˆ©æŸå¤±

### é…ç½®Cï¼ˆLLMä¸­ä»‹ï¼‰
- ç­–ç•¥mè¯¯å·®
- åŒ¿ååŒ–åŒ¹é…
- åˆ©æ¶¦æ•ˆç‡
- åˆ©æ¶¦æŸå¤±

### é…ç½®Dï¼ˆåŒè¾¹LLMï¼‰
- vsç†è®ºæœ€ä¼˜çš„åå·®
- äº¤äº’æŒ‡æ ‡ï¼ˆå‰¥å‰Šåº¦ï¼‰
- ç¦åˆ©æŸå¤±

---

## ğŸ¯ è¯„ä¼°å¤šä¸ªæ¨¡å‹

è¦è¯„ä¼°æ‰€æœ‰é…ç½®çš„æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹`__main__`å—ï¼Œå¾ªç¯æ‰€æœ‰æ¨¡å‹ï¼š

```python
# åœ¨ __main__ å—ä¸­ï¼Œæ›¿æ¢å•æ¨¡å‹è¯„ä¼°ä¸ºï¼š
for model_config in model_configs:
    model_name = model_config['config_name']
    print(f"\n{'='*70}")
    print(f"è¯„ä¼°æ¨¡å‹: {model_name}")
    print(f"{'='*70}")
    
    client = OpenAI(
        api_key=model_config['api_key'],
        **model_config.get('client_args', {})
    )
    
    llm_consumer = create_llm_consumer(client, model_config)
    llm_intermediary = create_llm_intermediary(client, model_config)
    
    # ... è¿è¡Œè¯„ä¼° ...
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. APIå¯†é’¥å®‰å…¨
- é…ç½®æ–‡ä»¶ä¸­åŒ…å«APIå¯†é’¥ï¼Œä¸è¦æäº¤åˆ°å…¬å¼€ä»“åº“
- å»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–.gitignore

### 2. APIè°ƒç”¨æˆæœ¬
- æ¯æ¬¡å®Œæ•´è¯„ä¼°çº¦éœ€è¦è°ƒç”¨ï¼š
  - é…ç½®Bï¼š20æ¬¡ï¼ˆæ¶ˆè´¹è€…å†³ç­–ï¼‰
  - é…ç½®Cï¼š1æ¬¡ï¼ˆä¸­ä»‹å†³ç­–ï¼‰
  - é…ç½®Dï¼š21æ¬¡ï¼ˆ1æ¬¡ä¸­ä»‹ + 20æ¬¡æ¶ˆè´¹è€…ï¼‰
  - æ€»è®¡ï¼š~42æ¬¡APIè°ƒç”¨
  
### 3. LLMå¤±è´¥å¤„ç†
- å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä¼šè‡ªåŠ¨å›é€€åˆ°ç®€å•çš„å¯å‘å¼è§„åˆ™
- ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š

### 4. æ¸©åº¦è®¾ç½®
- å½“å‰é…ç½®ä¸­`temperature=0`ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
- å¦‚æœæƒ³æµ‹è¯•LLMçš„ç¨³å®šæ€§ï¼Œå¯ä»¥è®¾ç½®`temperature > 0`

---

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
configs/
â””â”€â”€ model_configs.json              # æ¨¡å‹é…ç½®

src/evaluators/
â””â”€â”€ evaluate_scenario_c.py          # ä¸»è¯„ä¼°å™¨ï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰

evaluation_results/
â”œâ”€â”€ scenario_c_grok-3-mini_*.csv    # CSVæŠ¥å‘Š
â””â”€â”€ scenario_c_grok-3-mini_*.json   # è¯¦ç»†ç»“æœ

data/ground_truth/
â””â”€â”€ scenario_c_common_preferences_optimal.json  # Ground Truth
```

---

## ğŸ†š ä¸æµ‹è¯•è„šæœ¬çš„å¯¹æ¯”

| ç‰¹æ€§ | ç›´æ¥è¿è¡Œè¯„ä¼°å™¨ | test_scenario_c_evaluator.py |
|------|----------------|------------------------------|
| LLMç±»å‹ | çœŸå®APIè°ƒç”¨ | æ¨¡æ‹Ÿä»£ç† |
| é…ç½®æ¥æº | model_configs.json | ç¡¬ç¼–ç å‡½æ•° |
| æµ‹è¯•ç›®çš„ | çœŸå®è¯„ä¼° | éªŒè¯è¯„ä¼°å™¨åŠŸèƒ½ |
| é€‚ç”¨åœºæ™¯ | æ­£å¼è¯„ä¼° | å¼€å‘/è°ƒè¯• |

---

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### å¼€å‘é˜¶æ®µ
ä½¿ç”¨æµ‹è¯•è„šæœ¬ï¼ˆæ¨¡æ‹ŸLLMï¼‰ï¼š
```bash
python test_scenario_c_evaluator.py
```

### æ­£å¼è¯„ä¼°é˜¶æ®µ
ä½¿ç”¨çœŸå®LLMï¼š
```bash
python src/evaluators/evaluate_scenario_c.py
```

---

## ğŸ“ ç¤ºä¾‹è¾“å‡º

```
======================================================================
åœºæ™¯Cè¯„ä¼°å™¨ - ä½¿ç”¨çœŸå®LLMæ¨¡å‹
======================================================================

âœ… æˆåŠŸåŠ è½½æ¨¡å‹é…ç½®: configs/model_configs.json
å¯ç”¨æ¨¡å‹: ['grok-3-mini', 'gpt-4.1-mini', 'deepseek-v3', 'gemini-2.5-flash']

ä½¿ç”¨æ¨¡å‹: grok-3-mini

åˆ›å»ºLLMä»£ç†...
âœ… LLMä»£ç†åˆ›å»ºæˆåŠŸ

======================================================================
æ­¥éª¤1: åŠ è½½Ground Truth
======================================================================
âœ… æˆåŠŸåŠ è½½: data/ground_truth/scenario_c_common_preferences_optimal.json

ç†è®ºåŸºå‡†ï¼ˆé…ç½®Aï¼‰:
  m* = 0.5000
  anonymization* = anonymized
  r* = 0.0486
  ä¸­ä»‹åˆ©æ¶¦* = 1.5960

======================================================================
æ­¥éª¤2: è¯„ä¼°é…ç½®Bï¼ˆç†æ€§ä¸­ä»‹ Ã— grok-3-miniæ¶ˆè´¹è€…ï¼‰
======================================================================

...ï¼ˆè¯„ä¼°è¿‡ç¨‹ï¼‰...

å…³é”®æŒ‡æ ‡:
  å‚ä¸ç‡è¯¯å·®: 15.23%
  ä¸ªä½“å‡†ç¡®ç‡: 85.00%
  ç¦åˆ©æ¯”ç‡: 0.9512
  ç¦åˆ©æŸå¤±: 4.88%

...ï¼ˆç»§ç»­è¯„ä¼°é…ç½®Cå’ŒDï¼‰...

======================================================================
âœ… è¯„ä¼°å®Œæˆï¼
======================================================================

ğŸ“Š è¯„ä¼°æ¨¡å‹: grok-3-mini
ğŸ“ ç»“æœæ–‡ä»¶:
  â€¢ CSVæŠ¥å‘Š: evaluation_results/scenario_c_grok-3-mini_20260119_153000.csv
  â€¢ è¯¦ç»†JSON: evaluation_results/scenario_c_grok-3-mini_20260119_153000_detailed.json
```

---

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. æŸ¥çœ‹LLMçš„åŸå§‹å›ç­”

åœ¨`create_llm_consumer`æˆ–`create_llm_intermediary`ä¸­æ·»åŠ ï¼š

```python
answer = response.choices[0].message.content.strip()
print(f"[DEBUG] LLMå›ç­”: {answer}")  # æ·»åŠ è¿™è¡Œ
```

### 2. æµ‹è¯•å•ä¸ªæ¶ˆè´¹è€…

```python
# åœ¨__main__å—ä¸­ï¼Œè¯„ä¼°å‰æ·»åŠ ï¼š
test_consumer = evaluator._get_sample_consumers()[0]
test_decision = llm_consumer(test_consumer, 0.5, "anonymized")
print(f"æµ‹è¯•å†³ç­–: {test_decision}")
```

### 3. æ¯”è¾ƒå¤šæ¬¡è¿è¡Œ

ç”±äº`temperature=0`ï¼Œå¤šæ¬¡è¿è¡Œåº”è¯¥å¾—åˆ°ç›¸åŒç»“æœã€‚å¦‚æœä¸åŒï¼Œè¯´æ˜ï¼š
- LLMæœåŠ¡ä¸ç¨³å®š
- æç¤ºè¯éœ€è¦æ›´æ˜ç¡®
- è§£æé€»è¾‘éœ€è¦æ”¹è¿›

---

## âœ… éªŒè¯æ¸…å•

è¿è¡Œå‰ç¡®è®¤ï¼š
- [ ] Ground Truthå·²ç”Ÿæˆ
- [ ] `configs/model_configs.json`å­˜åœ¨ä¸”æœ‰æ•ˆ
- [ ] APIå¯†é’¥æœ‰æ•ˆä¸”æœ‰ä½™é¢
- [ ] ç½‘ç»œè¿æ¥æ­£å¸¸
- [ ] `openai` PythonåŒ…å·²å®‰è£…

```bash
pip install openai
```

---

**ç¥è¯„ä¼°é¡ºåˆ©ï¼** ğŸ‰
