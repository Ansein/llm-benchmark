"""
ä¸»è¯„ä¼°è„šæœ¬
æ‰¹é‡è¿è¡Œå¤šä¸ªæ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¯„ä¼°

# python run_evaluation.py --scenarios B --models grok-3-mini gpt-4.1-mini deepseek-v3 --num-trials 5 --max-iterations 15
"""

import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd

from src.evaluators import create_llm_client, ScenarioAEvaluator, ScenarioBEvaluator


def run_single_evaluation(
    scenario: str,
    model_name: str,
    num_trials: int = 3,
    max_iterations: int = 10,
    output_dir: str = "evaluation_results"
) -> Dict[str, Any]:
    """
    è¿è¡Œå•ä¸ªåœºæ™¯çš„è¯„ä¼°
    
    Args:
        scenario: åœºæ™¯åç§° ("A" æˆ– "B")
        model_name: æ¨¡å‹é…ç½®åç§°
        num_trials: æ¯ä¸ªå†³ç­–çš„é‡å¤æ¬¡æ•°
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"\n{'='*80}")
    print(f"ğŸš€ å¼€å§‹è¯„ä¼°: åœºæ™¯{scenario} | æ¨¡å‹: {model_name}")
    print(f"{'='*80}")
    
    try:
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        llm_client = create_llm_client(model_name)
        
        # æ ¹æ®åœºæ™¯é€‰æ‹©è¯„ä¼°å™¨
        if scenario == "A":
            evaluator = ScenarioAEvaluator(llm_client)
            # è¿è¡Œè¯„ä¼°
            results = evaluator.simulate_llm_equilibrium(
                num_trials=num_trials,
                max_iterations=max_iterations
            )
        elif scenario == "B":
            evaluator = ScenarioBEvaluator(llm_client)
            # è¿è¡Œè¯„ä¼°ï¼ˆå¹¶è¡Œåšå¼ˆæ¨¡å¼ï¼Œå‚æ•°åä¸ºmax_roundsï¼‰
            results = evaluator.simulate_llm_equilibrium(
                num_trials=num_trials,
                max_rounds=max_iterations
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åœºæ™¯: {scenario}")
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_evaluation_summary(results)
        
        # ä¿å­˜ç»“æœ
        output_path = Path(output_dir) / f"eval_scenario_{scenario}_{model_name}.json"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        evaluator.save_results(results, str(output_path))
        
        return results
    
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def run_batch_evaluation(
    scenarios: List[str],
    model_names: List[str],
    num_trials: int = 3,
    max_iterations: int = 10,
    output_dir: str = "evaluation_results"
):
    """
    æ‰¹é‡è¿è¡Œè¯„ä¼°
    
    Args:
        scenarios: åœºæ™¯åˆ—è¡¨ (["A", "B"])
        model_names: æ¨¡å‹åç§°åˆ—è¡¨
        num_trials: æ¯ä¸ªå†³ç­–çš„é‡å¤æ¬¡æ•°
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f"\n{'#'*80}")
    print(f"ğŸ¯ æ‰¹é‡è¯„ä¼°å¼€å§‹")
    print(f"{'#'*80}")
    print(f"åœºæ™¯: {scenarios}")
    print(f"æ¨¡å‹: {model_names}")
    print(f"æ¯ä¸ªå†³ç­–é‡å¤æ¬¡æ•°: {num_trials}")
    print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    all_results = []
    
    for scenario in scenarios:
        for model_name in model_names:
            result = run_single_evaluation(
                scenario=scenario,
                model_name=model_name,
                num_trials=num_trials,
                max_iterations=max_iterations,
                output_dir=output_dir
            )
            
            if result:
                all_results.append({
                    "scenario": scenario,
                    "model_name": model_name,
                    "result": result
                })
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_summary_report(all_results, output_dir)
    
    print(f"\n{'#'*80}")
    print(f"âœ… æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
    print(f"{'#'*80}")


def generate_summary_report(all_results: List[Dict[str, Any]], output_dir: str):
    """
    ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    
    Args:
        all_results: æ‰€æœ‰è¯„ä¼°ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
    print(f"{'='*80}")
    
    # å‡†å¤‡è¡¨æ ¼æ•°æ®
    summary_data = []
    
    for item in all_results:
        scenario = item["scenario"]
        model_name = item["model_name"]
        result = item["result"]
        
        metrics = result["metrics"]
        labels = result["labels"]
        
        row = {
            "åœºæ™¯": scenario,
            "æ¨¡å‹": model_name,
            "æ”¶æ•›": "âœ…" if result["converged"] else "âŒ",
            "è¿­ä»£æ¬¡æ•°": result["iterations"],
        }
        
        # åœºæ™¯Açš„æŒ‡æ ‡
        if scenario == "A":
            row.update({
                "æŠ«éœ²ç‡_LLM": f"{metrics['llm']['disclosure_rate']:.2%}",
                "æŠ«éœ²ç‡_GT": f"{metrics['ground_truth']['disclosure_rate']:.2%}",
                "åˆ©æ¶¦MAE": f"{metrics['deviations']['profit_mae']:.3f}",
                "CS_MAE": f"{metrics['deviations']['cs_mae']:.3f}",
                "ç¦åˆ©MAE": f"{metrics['deviations']['welfare_mae']:.3f}",
                "æŠ«éœ²ç‡åˆ†æ¡¶åŒ¹é…": "âœ…" if labels["llm_disclosure_rate_bucket"] == labels["gt_disclosure_rate_bucket"] else "âŒ",
                "è¿‡åº¦æŠ«éœ²åŒ¹é…": "âœ…" if labels["llm_over_disclosure"] == labels["gt_over_disclosure"] else "âŒ"
            })
        
        # åœºæ™¯Bçš„æŒ‡æ ‡
        elif scenario == "B":
            row.update({
                "åˆ†äº«ç‡_LLM": f"{metrics['llm']['share_rate']:.2%}",
                "åˆ†äº«ç‡_GT": f"{metrics['ground_truth']['share_rate']:.2%}",
                "åˆ©æ¶¦MAE": f"{metrics['deviations']['profit_mae']:.4f}",
                "ç¦åˆ©MAE": f"{metrics['deviations']['welfare_mae']:.4f}",
                "æ³„éœ²MAE": f"{metrics['deviations']['total_leakage_mae']:.4f}",
                "æ³„éœ²åˆ†æ¡¶åŒ¹é…": "âœ…" if labels["llm_leakage_bucket"] == labels["gt_leakage_bucket"] else "âŒ",
                "è¿‡åº¦åˆ†äº«åŒ¹é…": "âœ…" if labels["llm_over_sharing"] == labels["gt_over_sharing"] else "âŒ"
            })
        
        summary_data.append(row)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(summary_data)
    
    # æ‰“å°è¡¨æ ¼
    print("\n" + df.to_string(index=False))
    
    # ä¿å­˜ä¸ºCSV
    csv_path = Path(output_dir) / f"summary_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {csv_path}")
    
    # ä¿å­˜å®Œæ•´JSON
    json_path = Path(output_dir) / f"all_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {json_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¿è¡ŒLLM benchmarkè¯„ä¼°")
    
    parser.add_argument(
        "--scenarios",
        type=str,
        nargs="+",
        default=["A", "B"],
        choices=["A", "B"],
        help="è¦è¯„ä¼°çš„åœºæ™¯åˆ—è¡¨"
    )
    
    parser.add_argument(
        "--models",
        type=str,
        nargs="+",
        default=["gpt-4.1-mini"],
        help="è¦è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨ï¼ˆé…ç½®åç§°ï¼‰"
    )
    
    parser.add_argument(
        "--num-trials",
        type=int,
        default=3,
        help="æ¯ä¸ªå†³ç­–çš„é‡å¤æ¬¡æ•°ï¼ˆç”¨äºè¯„ä¼°ç¨³å®šæ€§ï¼‰"
    )
    
    parser.add_argument(
        "--max-iterations",
        type=int,
        default=10,
        help="å¯»æ‰¾å‡è¡¡çš„æœ€å¤§è¿­ä»£æ¬¡æ•°"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default="evaluation_results",
        help="è¾“å‡ºç›®å½•"
    )
    
    parser.add_argument(
        "--single",
        action="store_true",
        help="å•æ¬¡è¯„ä¼°æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )
    
    args = parser.parse_args()
    
    if args.single:
        # å•æ¬¡è¯„ä¼°æ¨¡å¼
        run_single_evaluation(
            scenario=args.scenarios[0],
            model_name=args.models[0],
            num_trials=args.num_trials,
            max_iterations=args.max_iterations,
            output_dir=args.output_dir
        )
    else:
        # æ‰¹é‡è¯„ä¼°æ¨¡å¼
        run_batch_evaluation(
            scenarios=args.scenarios,
            model_names=args.models,
            num_trials=args.num_trials,
            max_iterations=args.max_iterations,
            output_dir=args.output_dir
        )


if __name__ == "__main__":
    main()
